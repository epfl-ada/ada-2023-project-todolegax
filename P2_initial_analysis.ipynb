{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<h1 align=\"center\"><strong>Does size matter?</strong></h1>\n",
    "  <p align=\"center\">\n",
    "    Applied Data Analysis (CS-401)\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers are increasingly relying on **product rating** websites to inform their purchasing decisions. It has been demonstrated that when customers rate a product, they often exhibit a **tendency to be influenced by the previous ratings** of other customers, a phenomenon known as the **_herding effect_**.\n",
    "\n",
    "Despite this, an unresolved research question revolves around comprehending **how ratings might be impacted by the scale and the reputation of the vendor**. Utilizing data sourced from beer reviews websites, our objective is to investigate the **connection** between the **size and fame of vendors** (specifically, breweries) and **the perceived quality** of their products.\n",
    "\n",
    "Through the quantification of brewery size and popularity using **predefined metrics** and the **extraction of sentiment** from textual reviews, our aim is to ascertain whether a correlation exists between vendor size and notoriety and perceived product quality. Additionally, we plan to **explore the behaviors** of diverse consumer bases, considering **temporal dimensions** (how these phenomena have evolved over the years and seasons within the same year) and **spatial dimensions** (how these relationships differ across states and countries).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BeerAdvocate**: Project proposal and initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import  libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "import difflib\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data folder paths for BeerAdvocate\n",
    "DATA_FOLDER_BA = '../DATA/BeerAdvocate/'\n",
    "\n",
    "# Define the file paths for the datasets\n",
    "Beers_DATASET = DATA_FOLDER_BA+\"beers.csv\"\n",
    "Users_DATASET = DATA_FOLDER_BA+\"users.csv\"\n",
    "Reviews_DATASET = DATA_FOLDER_BA+\"reviews_BA.csv\"\n",
    "Breweries_DATASET = DATA_FOLDER_BA+\"breweries.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset exploration & cleaning\n",
    "\n",
    "The first step of the project consits in exploring the data available in the dataset as well as pre-processing them. This step consists mainly in handling the potential missing data and reformatting what needs to be reformated. The following subsections present this step for every datasets available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Beers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We starts with the dataset containing information about beers. We will probably not use this datatset since our project focuses more on the reviews and breweries but we decided to still perform some pre-processing on it in the case where we suddenly need it during the analysis of Milestone 3. The dataset includes the following columns:\n",
    "\n",
    "- `beer_id`: Identifier for the beer.\n",
    "- `beer_name`: Name of the beer.\n",
    "- `brewery_id`: Identifier for the brewery producing the beer.\n",
    "- `brewery_name`: Name of the brewery.\n",
    "- `style`: Beer style.\n",
    "- `nbr_ratings`: Number of ratings received.\n",
    "- `nbr_reviews`: Number of reviews.\n",
    "- `avg`: Average rating.\n",
    "- `ba_score`: BeerAdvocate score.\n",
    "- `bros_score`: Bros score.\n",
    "- `abv`: Alcohol by volume.\n",
    "- `avg_computed`: Computed average rating.\n",
    "- `zscore`: Z-score.\n",
    "- `nbr_matched_valid_ratings`: Number of matched valid ratings.\n",
    "- `avg_matched_valid_ratings`: Average of matched valid ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the datasets into Pandas DataFrames\n",
    "beers = pd.read_csv(Beers_DATASET)\n",
    "# Display 2 random chosen samples of the set\n",
    "display(beers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset is composed of 280'823 beers. However, we can see that some data are missing and that some beers have 0 reviews. Furthermore, we should check if there is some duplicates. To address these issues, the following pre-processing steps are applied:\n",
    "\n",
    "- **Filtering Beers with Less than 5 Reviews**: Deleting beers with fewer than 5 reviews, as they may not be characteristic.\n",
    "\n",
    "- **Handling Missing Values**: Dropping rows with NaN values in the `nbr_ratings` column.\n",
    "\n",
    "- **Removing Duplicates**: Dropping duplicate entries based on the `beer_name` and `beer_id` columns.\n",
    "\n",
    "- **Column Selection**: Dropping columns that won't be used in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a minimum threshold for the number of reviews\n",
    "MIN_NUMBER_OF_REVIEWS = 5\n",
    "\n",
    "# Create a filtered copy of the 'beers' DataFrame with a minimum number of reviews\n",
    "beers_filt = beers.copy(deep=True)\n",
    "beers_filt = beers_filt[beers_filt['nbr_reviews'] >= MIN_NUMBER_OF_REVIEWS]\n",
    "\n",
    "# Remove rows with missing values in the 'nbr_reviews' column\n",
    "beers_filt = beers_filt[beers_filt['nbr_reviews'].notna()]\n",
    "\n",
    "# Drop duplicate entries based on the 'beer_name' column\n",
    "beers_filt = beers_filt.drop_duplicates(subset=['beer_name'])\n",
    "\n",
    "# Calculate the number of duplicate entries based on 'beer_name' and 'beer_id'\n",
    "dupli_name = np.sum(beers_filt.duplicated(subset=['beer_name']))\n",
    "dupli_ID = np.sum(beers_filt.duplicated(subset=['beer_id']))\n",
    "\n",
    "# Drop specific columns from the filtered DataFrame\n",
    "beers_filt = beers_filt.drop(['zscore', 'nbr_matched_valid_ratings', 'avg_matched_valid_ratings', 'bros_score', 'ba_score'], axis=1)\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "display(beers_filt)\n",
    "\n",
    "# Print the number of duplicate entries for 'beer_name' and 'beer_id'\n",
    "print(f'Number of duplicate beer name = {dupli_name}')\n",
    "print(f'Number of duplicate beer ID = {dupli_ID}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, the number of beers drops to 42'923."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Users**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move to the pre-processing of the dataset containing information about users. The dataset includes the following columns:\n",
    "\n",
    "- `nbr_ratings`: Number of ratings made.\n",
    "- `nbr_reviews`: Number of reviews done.\n",
    "- `user_id`: Unique user identifier.\n",
    "- `user_name`: User Name.\n",
    "- `joined`:  Date of the sign up.\n",
    "- `location`: Location of the User.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the datasets into Pandas DataFrames\n",
    "users = pd.read_csv(Users_DATASET)\n",
    "display(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed of 153'704 users. We can again see that some data are missing and that some users did not gave any review. To address these issues our pre-processing involves the following steps:\n",
    "\n",
    "- **Filtering Users with 0 number of reviews**: Deleting users with 0 reviews, as they are not characteristic.\n",
    "\n",
    "- **Handling Missing Values**: Dropping rows with NaN values in the `nbr_reviews`, `user_id`, `user_name` and `location` columns.\n",
    "\n",
    "- **Check for Duplicated Users**: Check if there are multiple user with the same id.\n",
    "\n",
    "- **Formatting the date**: Reformat the date in the column `joined` in UTC format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the 'users' dataframe\n",
    "users_filt = users.copy(deep=True)\n",
    "\n",
    "# Check for duplicates based on user name and user ID\n",
    "dupli_name = np.sum(users_filt.duplicated(subset=['user_name']))\n",
    "dupli_ID = np.sum(users_filt.duplicated(subset=['user_id']))\n",
    "\n",
    "# Remove users with 0 reviews and NaN as the number of reviews\n",
    "users_filt = users_filt[users_filt['nbr_reviews'] >= 1]\n",
    "users_filt = users_filt[users_filt['nbr_reviews'].notna()]\n",
    "\n",
    "# Remove rows with NaN in 'user_id', 'user_name', and 'location'\n",
    "users_filt = users_filt[users_filt['user_id'].notna()]\n",
    "users_filt = users_filt[users_filt['user_name'].notna()]\n",
    "users_filt = users_filt[users_filt['location'].notna()]\n",
    "\n",
    "# Convert 'joined' column to datetime type\n",
    "users_filt['joined'] = users_filt['joined'].apply(lambda x: datetime.utcfromtimestamp(x) if not pd.isna(x) else x)\n",
    "\n",
    "# Rename the 'location' column to 'user_location'\n",
    "users_filt.rename(columns={'location': 'user_location'}, inplace=True)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "display(users_filt)\n",
    "\n",
    "# Display the number of duplicate user names and user IDs\n",
    "print(f'Number of duplicate user names = {dupli_name}')\n",
    "print(f'Number of duplicate user IDs = {dupli_ID}')\n",
    "\n",
    "# Display the number of NaN values in each category\n",
    "print('Number of NaN by category:')\n",
    "print(np.sum(users_filt.isna()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, a total of 58'199 users with complete information are kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Breweries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we conduct the pre-processing of the dataset containing information about breweries. The dataset comprises the following columns:\n",
    "\n",
    "- `id`: Brewery identifier.\n",
    "- `location`: Location of the brewery.\n",
    "- `name`: Name of the brewery.\n",
    "- `nbr_beers`: Number of beers produced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the datasets into Pandas DataFrames\n",
    "breweries = pd.read_csv(Breweries_DATASET)\n",
    "display(breweries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed of 16'758 breweries. We can directly see that some breweries have 0 beers and are therefore not relevant for our analysis. There might be some missing values as well, thus we decided to perform the following pre_processing steps:\n",
    "\n",
    "- **Filtering Breweries with 0 number of beers**: Deleting breweries with 0 beers, as they are not characteristic.\n",
    "\n",
    "- **Handling Missing Values**: Dropping rows with NaN values in the `nbr_beers`.\n",
    "\n",
    "- **Removing Duplicates**: Dropping duplicate entries based on the `name` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the 'breweries' dataframe\n",
    "breweries_filt = breweries.copy(deep=True)\n",
    "\n",
    "# Check for duplicates based on ID\n",
    "dupli_ID = np.sum(breweries_filt.duplicated(subset=['id']))\n",
    "\n",
    "# Remove breweries with 0 beers and NaN values\n",
    "breweries_filt = breweries_filt[breweries_filt['nbr_beers'] >= 1]\n",
    "breweries_filt = breweries_filt[breweries_filt.notna()]\n",
    "\n",
    "# Remove duplicate entries based on brewery name\n",
    "breweries_filt = breweries_filt.drop_duplicates(subset='name')\n",
    "\n",
    "# Check for duplicates based on name\n",
    "dupli_name = np.sum(breweries_filt.duplicated(subset=['name']))\n",
    "\n",
    "# Rename columns for consistency\n",
    "breweries_filt.rename(columns={'name': 'brewery_name', 'id': 'brewery_id', 'location': 'brewery_location'}, inplace=True)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "display(breweries_filt)\n",
    "\n",
    "# Display the number of duplicate names and IDs\n",
    "print(f'Number of duplicate names = {dupli_name}')\n",
    "print(f'Number of duplicate IDs = {dupli_ID}')\n",
    "\n",
    "# Display the number of NaN values in each category\n",
    "print('Number of NaN by category:')\n",
    "print(np.sum(breweries_filt.isna()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, 14'158 breweries are remaining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now delve into the BeerAdvocate Reviews dataset, focusing on reviews of various beers. The dataset contains the following columns:\n",
    "\n",
    "- `beer_name`: Name of the beer.\n",
    "- `beer_id`: Identifier for the beer.\n",
    "- `brewery_name`: Name of the brewery producing the beer.\n",
    "- `brewery_id`: Identifier for the brewery.\n",
    "- `style`: Beer style.\n",
    "- `abv`: Alcohol by volume.\n",
    "- `date`: Timestamp of the review.\n",
    "- `user_name`: Username of the reviewer.\n",
    "- `user_id`: Identifier for the user.\n",
    "- `appearance`: Rating for the beer's appearance.\n",
    "- `aroma`: Rating for the beer's aroma.\n",
    "- `palate`: Rating for the beer's palate.\n",
    "- `taste`: Rating for the beer's taste.\n",
    "- `overall`: Overall rating.\n",
    "- `rating`: Overall user rating.\n",
    "- `text`: Review text.\n",
    "\n",
    "Since the .txt file containing the reviews is huge, we first converted it into smaller files that we then regrouped in a .csv file. The code in `split_reviews.py` was used or this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Reviews dataset into a pandas DataFrame.\n",
    "reviews_BA = pd.read_csv(Reviews_DATASET)\n",
    "display(reviews_BA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pre-processing begins with the following initial steps:\n",
    "\n",
    "- **Converting Timestamps to Datetime**: We start by converting the 'date' column, which contains timestamps, into the datetime format. This conversion enables us to perform time-based analyses more effectively.\n",
    "\n",
    "- **Handling Missing Values**: We address missing values in the dataset by dropping rows with NaN values. This ensures that our analysis is based on complete and reliable data.\n",
    "\n",
    "- **Removing Unnecessary Column**: The 'abv' column, representing the alcohol by volume, is not useful for our specific analysis. Consequently, we opt to drop this column to streamline our dataset.\n",
    "\n",
    "- **Removing White Space before and after Stings**: Some strings columns like `user_name` begin with a white space, which is a problem for merging. Consequently we need to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the reviews_BA DataFrame to avoid modifying the original DataFrame.\n",
    "reviews_filt = reviews_BA.copy(deep=True)\n",
    "# Convert the 'date' column to a datetime format.\n",
    "# If the 'date' value is not NaN, apply the conversion using utcfromtimestamp.\n",
    "# If the 'date' value is NaN, leave it unchanged.\n",
    "reviews_filt['date'] = reviews_filt['date'].apply(lambda x: datetime.utcfromtimestamp(x) if not pd.isna(x) else x)\n",
    "# Drop rows where the 'text' column has NaN values.\n",
    "reviews_filt = reviews_filt[reviews_filt['text'].notna()]\n",
    "# Drop the 'abv' column from the reviews_filt DataFrame.\n",
    "reviews_filt = reviews_filt.drop(['abv'], axis=1)\n",
    "# Remove leading and trailing whitespaces (if they exist) from the following columns: \n",
    "# user_id, user_name, beer_name, brewery_name, style, and text.\n",
    "reviews_filt.user_id = reviews_filt.user_id.apply(lambda x: x.strip())\n",
    "reviews_filt.user_name = reviews_filt.user_name.astype(str).apply(lambda x: x.strip())\n",
    "reviews_filt.beer_name = reviews_filt.beer_name.apply(lambda x: x.strip())\n",
    "reviews_filt.brewery_name = reviews_filt.brewery_name.apply(lambda x: x.strip())\n",
    "reviews_filt['style'] = reviews_filt['style'].apply(lambda x: x.strip())\n",
    "reviews_filt.text = reviews_filt.text.apply(lambda x: x.strip())\n",
    "# Display the updated reviews_filt DataFrame.\n",
    "display(reviews_filt)\n",
    "# Print the number of NaN values for each column in the reviews_filt DataFrame.\n",
    "print('Number of NaN by category:')\n",
    "print(np.sum(reviews_filt.isna()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now aim to visualize the distribution of the length of the reviews to:\n",
    "\n",
    "- **Get insight into review length variation**: Visualizing the distribution allows us to understand the range and variability in review lengths. Some reviews may be succinct, while others may be more detailed.\n",
    "\n",
    "- **Assess data quality**: Analyzing review lengths can also serve as a quality check. Unusually short or long reviews may warrant further investigation to ensure data integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the review lengths using the 'text' column from the reviews_filt DataFrame.\n",
    "plt.hist(reviews_filt['text'].str.len(), bins=200, log=True)\n",
    "\n",
    "# Set x-axis and y-axis labels and the title.\n",
    "plt.xlabel('Review length')\n",
    "plt.ylabel('Number of reviews')\n",
    "plt.title('Distribution of Review Length')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BeerAdvocate website advises to create reviews of at list 150 characters. It can be seen that not all the reviews have at least 150 characters. We will therefore remove them in the next steps to keep only relevant revies. The statistics of the review length are displayed in the next cell. As we can see, the median is at 580 characters. The distribution is skewed though, with a small quantity of reviews being more than 5000 characters. This will be interesting to analyse in the next milestone if there is a link between the length of the reviews and the scale or popularity of a brewery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics of the review lengths.\n",
    "reviews_filt['text'].str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted to improve consistency by **translating** all non-English textual reviews. To this end, we used the language detection module, $\\texttt{detect}$, of the $\\texttt{langdetect}$ library to **initially identify the language of each review**.\n",
    "\n",
    "Due to the considerable computation time required for language detection, we decided to **keep the language identifier** of each review in a separate dataset, together with the **unique identifiers** of the **beer** and the **user**.\n",
    "\n",
    "This approach allows us to store the language information in our archive, facilitating efficient access without the need to calculate the language detection for each review each time.\n",
    "\n",
    "Note that if the CSV file containing `user_id`, `beer_id` and `text_lang` (the language identifier of the reviews) exists in the repository, we can avoid recomputing the information. Instead, we can merge the review dataset with this auxiliary dataset into a consolidated dataset, simplifying our analysis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next sections will need these modules to be run\n",
    "# Importing the 'unescape' function from the 'html' module for text cleaning of html escape characters\n",
    "try:\n",
    "    from html import unescape\n",
    "except:\n",
    "    !pip install html\n",
    "\n",
    "# Importing the 'detect' function from the 'langdetect' module for language detection of reviews\n",
    "try:\n",
    "    from langdetect import detect\n",
    "except:\n",
    "    !pip install langdetect\n",
    "\n",
    "# Importing the 'GoogleTranslator' from the 'deep_translatore' module for reviews translation\n",
    "try:\n",
    "    from deep_translator import GoogleTranslator\n",
    "except:\n",
    "    !pip install deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the .csv file exists then we don't redo the detection\n",
    "data_name = 'reviews_lang.csv'\n",
    "CODE_ERROR = 'Error'\n",
    "\n",
    "if os.path.exists(data_name):\n",
    "    reviews_language = pd.read_csv(data_name)\n",
    "    reviews_filt = pd.merge(reviews_filt, reviews_language, on=['beer_id','user_id'], how='left')\n",
    "else:\n",
    "    # Detect the language of each review. Handle exception for non corrected reviews.\n",
    "    text_lang = []\n",
    "    for review in reviews_filt['text']:\n",
    "        try:\n",
    "            text_lang.append(detect(review))\n",
    "        except:\n",
    "            text_lang.append(CODE_ERROR)\n",
    "            continue\n",
    "    \n",
    "    # Adding a new column 'text_lang' to store the detected language for each review\n",
    "    reviews_filt['text_lang'] = pd.Series(text_lang)\n",
    "\n",
    "    # Store the language information in a  \n",
    "    reviews_filt[['beer_id', 'user_id', 'text_lang']].to_csv(data_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the consolidated dataset with the new column giving the language of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 3 randomly chosen rows of the new consolidated dataset\n",
    "display(reviews_filt.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check if errors occured during the detection, as well as if NaN values appeared in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of errors detected are:', np.sum(reviews_filt['text_lang'] == CODE_ERROR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of NaN values in text_lang colum is:', np.sum(reviews_filt['text_lang'].isna()), '/', len(reviews_filt['text_lang']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then decide to drop the NaN values due to the small number of occurrences of NaN values in the `text_lang` column in our filtered reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt = reviews_filt[reviews_filt['text_lang'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now have a look to the variety of languages composing the reviews. As expected a vast majority of them are in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The count and variety of distinct languages used in the reviews within our dataset:')\n",
    "print(reviews_filt['text_lang'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then filter rows in the DataFrame where the `text_lang` column is not 'en', and then apply translation to English for the corresponding 'text' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Translates the input text to English.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to be translated.\n",
    "        source (str): The source language (default is 'auto' for automatic detection).\n",
    "        target (str): The target language (default is 'en' for English).\n",
    "\n",
    "    Returns:\n",
    "        The translated text in English.\n",
    "\"\"\"\n",
    "def translate_to_english(text, source='auto', target='en'):\n",
    "    translated = GoogleTranslator(source=source, target=target).translate(text)\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt[reviews_filt.text_lang != 'en']['text'] = reviews_filt[reviews_filt.text_lang != 'en']['text'].apply(lambda x: translate_to_english(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all reviews now in **English**, we can proceed to remove the `text_lang` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt = reviews_filt.drop(['text_lang'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then undertook further pre-processing, focusing on the textual representation of reviews. We followed the following steps:\n",
    "\n",
    "- **Management of special characters**: After examining the modified and translated dataset, we observed the presence of some special characters such as \"\\&quot;\" and \"\\x92\" in some reviews. To solve this problem, we used the html.unescape function to convert the HTML entities and then removed the non-ASCII characters by encoding them in ASCII and decoding them again.\n",
    "\n",
    "- **Filtering short reviews**: As a final step, we filtered out reviews with less than 150 characters. This step aimed to exclude shorter reviews from our dataset, focusing on more substantial texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 'unescape' function to decode HTML entities in the 'text' column\n",
    "reviews_filt['text'] = reviews_filt['text'].apply(unescape)\n",
    "\n",
    "# Remove non-ASCII characters by encoding to ASCII and decoding back\n",
    "reviews_filt['text'] = reviews_filt['text'].apply(lambda x: x.encode('ascii', 'ignore').decode())\n",
    "\n",
    "# Set the minimum number of characters for reviews\n",
    "MIN_NUMBER_OF_CHARACTER = 150\n",
    "\n",
    "# Filter out reviews with fewer than 'min_character' characters\n",
    "reviews_filt = reviews_filt[reviews_filt['text'].str.len() > MIN_NUMBER_OF_CHARACTER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reviews_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with 2'587'598 reviews. Now that the datasets are ready, we can start with some preliminary analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Metrics Definition**\n",
    "In our analysis, we introduce **two key metrics** to quantitatively **assess the characteristics of breweries**: Size Metrics and Popularity Metrics. These metrics are created in the next cells. Since we don't have data about the revenue or number of liter produced by the brewery, we decided to base ourself on variable present in the dataset to construct these metrics. Some verification will be done by hand, by searching information about some breweries on the web to assess the quality of the metrics and to correct the coefficients if needed.\n",
    "\n",
    "####  **Size Metrics**\n",
    "To numerically evaluate the size of a brewery, we built an index based on the following formula:\n",
    "\n",
    "$$ \\text{Size} = \\alpha N_r + \\beta N_b + \\gamma N_t$$\n",
    "\n",
    "With:\n",
    "- $N_r =$ number of reviews normalized by the total number of reviews\n",
    "- $N_b =$ number of beers produced normalized by the total number of beers\n",
    "- $N_t =$ number of different types (style) of beer produced normalized by the total number of styles\n",
    "\n",
    "The coefficients $\\alpha, \\beta, \\gamma$ are chosen by hand. \n",
    "\n",
    "#### **Popularity Metrics**\n",
    "$$ \\text{Popularity} = \\dfrac{N_r}{N_b}$$\n",
    "\n",
    "With $N_r =$ number of reviews, $N_b =$ number of beers produced\n",
    "\n",
    "\n",
    "Note that both metrics are normalized to get a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract brewery_id and style columns from reviews_filt and merge with breweries_filt to get the type of the beer reviewed\n",
    "review_brew = reviews_filt[['brewery_id', 'style']].copy(deep=True)\n",
    "review_brew = pd.merge(review_brew, breweries_filt, on=['brewery_id'], how='inner')\n",
    "\n",
    "# Drop brewery location column from the merged DataFrame as we're not planning to use it\n",
    "review_brew = review_brew.drop(['brewery_location'], axis=1)\n",
    "\n",
    "# Count the number of reviews for each brewery\n",
    "brew_review_count = review_brew.groupby('brewery_id').size().reset_index(name='nbr_reviews')\n",
    "\n",
    "# Count the number of unique styles for each brewery\n",
    "brew_style_count = review_brew.groupby('brewery_id')['style'].unique().reset_index(name='styles')\n",
    "brew_style_count['nbr_styles'] = brew_style_count['styles'].apply(lambda x: len(x))\n",
    "\n",
    "# Merge the review count, style count, and selected columns from breweries_filt\n",
    "df_metrics = pd.merge(brew_review_count, brew_style_count[['brewery_id', 'nbr_styles']], on=['brewery_id'], how='inner')\n",
    "df_metrics = pd.merge(df_metrics, breweries_filt[['brewery_id', 'brewery_name', 'nbr_beers']], on=['brewery_id'], how='inner')\n",
    "\n",
    "# Compute size metrics\n",
    "tot_nb_reviews = df_metrics['nbr_reviews'].sum()\n",
    "tot_nb_beers = df_metrics['nbr_beers'].sum()\n",
    "tot_nb_styles = df_metrics['nbr_styles'].sum()\n",
    "# Choose coeff. aplha, beta, gamma for size metrics\n",
    "alpha = 3\n",
    "beta = 1\n",
    "gamma = 1\n",
    "\n",
    "# Calculate size metrics based on the weighted sum of reviews, beers, and styles\n",
    "df_metrics['size_metrics'] = df_metrics.apply(lambda row: alpha * row['nbr_reviews'] / tot_nb_reviews + beta * row['nbr_beers'] / tot_nb_beers + gamma * row['nbr_styles'] / tot_nb_styles, axis=1)\n",
    "\n",
    "# Normalize size metrics to ensure values are between 0 and 1\n",
    "df_metrics['size_metrics'] = df_metrics['size_metrics'] / df_metrics['size_metrics'].max()\n",
    "\n",
    "# Calculate popularity metrics based on the ratio of reviews to beers\n",
    "df_metrics['popularity_metrics'] = df_metrics.apply(lambda row: (row['nbr_reviews'] / tot_nb_reviews) / (row['nbr_beers'] / tot_nb_beers), axis=1)\n",
    "\n",
    "# Normalize popularity metrics to ensure values are between 0 and 1\n",
    "df_metrics['popularity_metrics'] = df_metrics['popularity_metrics'] / df_metrics['popularity_metrics'].max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at the metrics, we can have a look to the distribution of the variable used to compute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Plot histogram for '# reviews' in the first subplot\n",
    "axs[0].hist(df_metrics['nbr_reviews'], bins=300, color='blue', alpha=0.7)\n",
    "axs[0].set_title('Distribution of the breweries \\n based on number of reviews')\n",
    "axs[0].set_xlabel('Number of Reviews')\n",
    "axs[0].set_ylabel('Number of breweries')\n",
    "\n",
    "# Plot histogram for '# beers' in the second subplot\n",
    "axs[1].hist(df_metrics['nbr_beers'], bins=300, color='green', alpha=0.7)\n",
    "axs[1].set_title('Distribution of the breweries \\n based on number of different beers')\n",
    "axs[1].set_xlabel('Number of Different Beers')\n",
    "axs[1].set_ylabel('Number of breweries')\n",
    "\n",
    "# Plot histogram for '# style' in the third subplot\n",
    "axs[2].hist(df_metrics['nbr_styles'], bins=50, color='red', alpha=0.7)\n",
    "axs[2].set_title('Distribution of the breweries \\n based on number of beer styles')\n",
    "axs[2].set_xlabel('Number of Different Styles')\n",
    "axs[2].set_ylabel('Number of breweries')\n",
    "\n",
    "# Set y-axis scale to logarithmic for better visibility of distribution in all subplots\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[2].set_yscale('log')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all the variables have similar distribution. The number of reviews seems to be more skewed though, with a few breweries having a lot more reviews. Note that the y-axis scale is set to **logarithmic** for better visualization of the distribution, and the layout is adjusted for better presentation. We can now have a look to how the breweries are distributed based on the metrics calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot histogram for 'size metrics' in the first subplot\n",
    "axs[0].hist(df_metrics['size_metrics'], bins=300, color='blue', alpha=0.7)\n",
    "axs[0].set_title('Distribution of the breweries based on size metrics')\n",
    "axs[0].set_xlabel('Size metrics')\n",
    "axs[0].set_ylabel('Number of breweries')\n",
    "\n",
    "# Plot histogram for 'popularity metrics' in the second subplot\n",
    "axs[1].hist(df_metrics['popularity_metrics'], bins=300, color='green', alpha=0.7)\n",
    "axs[1].set_title('Distribution of the breweries based on popularity metrics')\n",
    "axs[1].set_xlabel('Popularity metrics')\n",
    "axs[1].set_ylabel('Number of breweries')\n",
    "\n",
    "# Set y-axis scale to logarithmic for better visibility of distribution in both subplots\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the same skew distribution for both metrics. But let's see if the biggest breweries are the also the most popular ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort breweries by size\n",
    "size_sorted = df_metrics.sort_values(by='size_metrics', ascending=False).reset_index()\n",
    "size_sorted.index = np.linspace(1, len(size_sorted), len(size_sorted)).astype(int)\n",
    "print(\"Overview of the size metrics ranking:\")\n",
    "\n",
    "# Display a summary of the top breweries based on size metrics\n",
    "display(size_sorted[['brewery_name', 'size_metrics', 'nbr_reviews', 'nbr_beers', 'nbr_styles']].iloc[[0, 1, 2, 9, 99, 999, 9999]])\n",
    "\n",
    "\n",
    "# Sort breweries by popularity\n",
    "popu_sorted = df_metrics.sort_values(by='popularity_metrics', ascending=False).reset_index()\n",
    "popu_sorted.index = np.linspace(1, len(popu_sorted), len(popu_sorted)).astype(int)\n",
    "print(\"Overview of the popularity metrics ranking:\")\n",
    "\n",
    "# Display a summary of the top breweries based on popularity metrics\n",
    "display(popu_sorted[['brewery_name', 'popularity_metrics', 'nbr_reviews', 'nbr_beers']].iloc[[0, 1, 2, 9, 99, 999, 9999]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sorting the breweries by size, it becomes evident that the top-ranking breweries are primarily located in the US. This observation aligns with expectations, considering that the majority of reviewers and breweries are located in the US (as detailed in the following section).\n",
    "\n",
    "Looking for the 3 biggest and the 100th on the web, we can extract their beer production:\n",
    "\n",
    "- (1st) Boston Beer Company: 5,300,000 barrels (6,200,000 hL) found under https://en.wikipedia.org/wiki/Boston_Beer_Company\n",
    "- (2nd) Sierra Nevada Brewing Co.: 1,250,000 barrels (510,000 hL) found under https://en.wikipedia.org/wiki/Sierra_Nevada_Brewing_Company\n",
    "- (3rd) Stone Brewing: 325,645 barrels (382,000 hL) found under https://en.wikipedia.org/wiki/Stone_Brewing_Co.\n",
    "- (100th) Uinta Brewing Company: 77'000 barrels found under https://en.wikipedia.org/wiki/Uinta_Brewing_Company\n",
    "\n",
    "Given these values, it seems that the size metrics works, at least for the top ranked breweries.\n",
    "\n",
    "On the other side, the popularity metrics yield significantly different results. The top-ranked breweries include historically renowned Belgian breweries such as \"Brasserie de Rochefort,\" notable for its \"Trappist\" beer, and \"Brasserie d'Orval.\" This highlights that the popularity metric captures a different aspect of brewery influence, focusing on factors beyond sheer size.\n",
    "\n",
    "It is important to note that this preliminary analysis does not take into account the time evolution. It will be interesting to see how it has evolved over the years. Some breweries for example could have significantly changed size over the years. However this pre-analysis validates the choice of the metrics and opens a large range of possibilities for the next analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Geographical Analysis**\n",
    "\n",
    "The goal of this section is to gain insights into the geographical distribution of both breweries and reviewers within the dataset. Ultimately, we aim to calculate the distances between breweries and their respective reviewers. This analysis could potentially unveil distinctions between brewery types, revealing whether certain types of breweries attract predominantly local reviewers or have a more globally dispersed audience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysis of Brewery Location**\n",
    "\n",
    "We initiate our analysis by examining the geographical distribution of breweries in the dataset. To achieve this, we integrate the dataset with a map sourced from Geopandas (https://www.naturalearthdata.com/downloads/110m-cultural-vectors/).\n",
    "\n",
    "Ensuring alignment between the country names used in the map and those in the brewery dataset is crucial. To address this, we calculate the Hamming distance between them and substitute the brewery location with the closest match. In instances where no match is found, we opt to eliminate the corresponding brewery. This scenario applies to 26 breweries out of over 14,000, which is deemed acceptable for this level of analysis. It's worth noting that some removed breweries had HTTP links as their location, explaining the lack of match with the map.\n",
    "\n",
    "For breweries located in the USA, the dataset includes information about the state. Consequently, we extend the same process to the states in the United States of America. To facilitate this, we introduce a new column, 'state,' in the dataframe. This allows us to split the location of US breweries into 'country' and 'state.' \n",
    "\n",
    "The geodataframe for the USA can be accessed here: https://eric.clst.org/tech/usgeojson/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamming distance function\n",
    "def find_closest_match(input_str, target_list):\n",
    "    \"\"\"Find the closest match in a list using Hamming distance.\"\"\"\n",
    "    return difflib.get_close_matches(input_str, target_list, n=1, cutoff=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make deep copy of breweries filtered dataframe reset indexes (needed for the loop next)\n",
    "breweries_loc = breweries_filt.copy(deep=True)\n",
    "breweries_loc.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Load the world map shapefile\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# Load the USA state map\n",
    "us_states = gpd.read_file('GeoJSON/gz_2010_us_040_00_500k.json')\n",
    "\n",
    "# Extract countries name\n",
    "unique_loc = world['name'].unique()\n",
    "# Extract state name\n",
    "unique_state = us_states['NAME'].unique()\n",
    "\n",
    "# Create column for state in breweries df, will be used for USA\n",
    "breweries_loc['brewery_state'] = '-'\n",
    "\n",
    "# Init dropped breweries\n",
    "dropped_brew = []\n",
    "\n",
    "# Loop through the 'brewery_location' column and replace values if needed\n",
    "for i, location in enumerate(breweries_loc['brewery_location']):\n",
    "    # Check if in USA --> (USA, state)\n",
    "    try:\n",
    "        location, state = location.split(', ')\n",
    "    except:\n",
    "        state = location  # if not in USA, state = location\n",
    "    \n",
    "    # Find closest match with Hamming distance\n",
    "    closest_match_country = find_closest_match(location, unique_loc)\n",
    "    \n",
    "    if closest_match_country:\n",
    "        # If closest match is USA --> match state      \n",
    "        if closest_match_country[0] == 'United States of America':\n",
    "            breweries_loc.at[i, 'brewery_location'] = closest_match_country[0]\n",
    "            \n",
    "            # Find state\n",
    "            closest_match_state = find_closest_match(state, unique_state)\n",
    "            if closest_match_state:\n",
    "                breweries_loc.at[i, 'brewery_state'] = closest_match_state[0]\n",
    "            else:\n",
    "                breweries_loc = breweries_loc.drop(index=i)\n",
    "        else:\n",
    "            # If not in USA --> state = location \n",
    "            breweries_loc.at[i, 'brewery_location'] = closest_match_country[0]\n",
    "            breweries_loc.at[i, 'brewery_state'] = closest_match_country[0]\n",
    "    else:\n",
    "        #print(i)\n",
    "        #print(breweries_loc.at[i, 'brewery_location'])\n",
    "        # If no match, drop\n",
    "        dropped_brew.append(location)\n",
    "        breweries_loc = breweries_loc.drop(index=i)\n",
    "\n",
    "# Display a sample of the modified dataframe\n",
    "display(breweries_loc.sample(3))\n",
    "\n",
    "# Print the number of dropped breweries and their locations\n",
    "print(f'{len(dropped_brew)} breweries were dropped, at the following location:')\n",
    "print(dropped_brew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore the **Top 10 countries** with the **highest number of breweries** to gain insights into the **global distribution of brewing establishments**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of breweries in each country\n",
    "brewery_counts = breweries_loc['brewery_location'].value_counts().reset_index()\n",
    "brewery_counts.columns = ['brewery_location', 'nb_breweries']\n",
    "\n",
    "# Identify the top 10 countries\n",
    "top10 = brewery_counts.sort_values(by='nb_breweries', ascending=False).head(10)\n",
    "top10 = top10[['brewery_location', 'nb_breweries']]\n",
    "top10.index = np.linspace(1, 10, 10).astype(int)\n",
    "top10.nb_breweries = top10.nb_breweries.astype(int)\n",
    "\n",
    "# Display the Top 10 countries with the most breweries\n",
    "print('Top 10 countries with the most breweries:')\n",
    "display(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the data, **there are significantly more breweries in the US** compared to other countries. Due to this notable concentration, we will consider the breweries by states in the next section. To facilitate this, we extend the world geodataframe by incorporating the US state geodataframe in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add US states to world df\n",
    "us_states.rename(columns={'NAME': 'name'}, inplace = True)\n",
    "world_with_US_states = pd.concat([world[['name','geometry']],us_states[['name','geometry']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look again to the Top 10 countries with the highest number of breweries. However, this time, we'll consider **each US state as a distinct 'country'.** Additionally, we'll **visualize the global distribution of breweries** to gain a comprehensive understanding of their geographical spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of breweries in each country\n",
    "brewery_counts = breweries_loc['brewery_state'].value_counts().reset_index()\n",
    "brewery_counts.columns = ['brewery_state', 'nb_breweries']\n",
    "\n",
    "# Merge brewery counts with the world map data\n",
    "world_merge = world_with_US_states.merge(brewery_counts, how='left', left_on='name', right_on='brewery_state')\n",
    "\n",
    "# Fill NaN values (countries without breweries) with 0\n",
    "world_merge['nb_breweries'].fillna(0, inplace=True)\n",
    "# Fill NaN values of state with Unknown\n",
    "world_merge['brewery_state'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Find top 10 countries\n",
    "top10 = world_merge.sort_values(by='nb_breweries', ascending=False).head(10)\n",
    "top10 = top10[['name', 'nb_breweries']]\n",
    "top10.index = np.linspace(1, 10, 10).astype(int)\n",
    "top10.nb_breweries = top10.nb_breweries.astype(int)\n",
    "\n",
    "# Display the Top 10 countries (considering US states) with the most breweries\n",
    "print('Top 10 countries (considering US states) with the most breweries:')\n",
    "display(top10)\n",
    "\n",
    "# Plot the choropleth map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 7))\n",
    "world_merge.boundary.plot(ax=ax, color='black', linewidth=0.5)\n",
    "world_merge.plot(column='nb_breweries', ax=ax, legend=True,\n",
    "                 norm=LogNorm(vmin=1, vmax=world_merge['nb_breweries'].max()),\n",
    "                 legend_kwds={'label': \"Number of Breweries by Country\",\n",
    "                              'orientation': \"vertical\"},\n",
    "                 cmap='Reds')\n",
    "\n",
    "# Remove the axis\n",
    "ax.set_title('Distribution of the breweries in the World')\n",
    "ax.set_axis_off()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The revised analysis reveals distinct results. Germany takes the lead, followed by Finland, and several US states are now present in the Top 10. The conclusion of this analysis is that there are many more breweries in the US than anywhere else in the world. Acknowledging this, it becomes essential to consider the US's internal diversity in subsequent analyses. Given the availability of information about individual US states, incorporating them into the analysis could provide valuable insights, offering a more nuanced understanding of the brewing landscape within the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysis of User Location**\n",
    "Now that we have gained insights into the distribution of breweries worldwide, we can apply a **similar analysis** to the **user data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the filtered users dataset and reset indexes (needed for the loop next)\n",
    "users_loc = users_filt.copy(deep=True)\n",
    "users_loc.reset_index(inplace=True,drop=True)\n",
    "# Create a column for the state in the users dataframe, to be used for USA\n",
    "users_loc['user_state'] = '-'\n",
    "\n",
    "# Initialize a list for dropped user locations\n",
    "dropped_user = []\n",
    "\n",
    "# Loop through the 'user_location' column and replace values if needed\n",
    "for i, location in enumerate(users_loc['user_location']):\n",
    "    try:\n",
    "        location, state = location.split(', ')\n",
    "    except:\n",
    "        state = location\n",
    "    \n",
    "    # Find the closest match using Hamming distance\n",
    "    closest_match_country = find_closest_match(location, unique_loc)\n",
    "\n",
    "    # If there is a match\n",
    "    if closest_match_country:\n",
    "        if closest_match_country[0] == 'United States of America':\n",
    "            users_loc.at[i, 'user_location'] = closest_match_country[0]\n",
    "            \n",
    "            # Find the state\n",
    "            closest_match_state = find_closest_match(state, unique_state)\n",
    "            if closest_match_state:\n",
    "                users_loc.at[i, 'user_state'] = closest_match_state[0]\n",
    "            else:\n",
    "                users_loc = users_loc.drop(index=i)\n",
    "        else:\n",
    "            # Replace the value in the DataFrame\n",
    "            users_loc.at[i, 'user_location'] = closest_match_country[0]\n",
    "            users_loc.at[i, 'user_state'] = closest_match_country[0]\n",
    "    # No match, drop the row\n",
    "    else:\n",
    "        dropped_user.append(location)\n",
    "        users_loc = users_loc.drop(index=i)\n",
    "\n",
    "# Display a sample of the modified users dataframe\n",
    "display(users_loc.sample(3))\n",
    "\n",
    "# Print the number of dropped users and their locations\n",
    "print(f'{len(dropped_user)} users were dropped, at the following location:')\n",
    "print(dropped_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the **Top 10 countries** with the **highest number of reviewers** to gain insights into the global distribution of reviewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of reviewers in each country\n",
    "reviewers_counts = users_loc['user_location'].value_counts().reset_index()\n",
    "reviewers_counts.columns = ['user_location', 'nb_reviewers']\n",
    "\n",
    "# Identify the top 10 countries according to the number of reviewers\n",
    "top10 = reviewers_counts.sort_values(by='nb_reviewers', ascending=False).head(10)\n",
    "top10 = top10[['user_location', 'nb_reviewers']]\n",
    "top10.index = np.linspace(1, 10, 10).astype(int)\n",
    "top10.nb_reviewers = top10.nb_reviewers.astype(int)\n",
    "\n",
    "# Display the Top 10 countries with the most reviewers\n",
    "print('Top 10 countries with the most reviewers:')\n",
    "display(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, there is once again a **substantial number of reviewers in the US**. In the next section, we will **replicate the analysis**, this time considering reviewers based on **US states**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of reviewers in each country\n",
    "reviewers_counts = users_loc['user_state'].value_counts().reset_index()\n",
    "reviewers_counts.columns = ['user_state', 'nb_reviewers']\n",
    "\n",
    "# Merge reviewer counts with the world map data\n",
    "world_merge = world_with_US_states.merge(reviewers_counts, how='left', left_on='name', right_on='user_state')\n",
    "\n",
    "# Fill NaN values (countries without reviewers) with 0\n",
    "world_merge['nb_reviewers'].fillna(0, inplace=True)\n",
    "world_merge['user_state'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Identify the top 10 countries\n",
    "top10 = world_merge.sort_values(by='nb_reviewers', ascending=False).head(10)\n",
    "top10 = top10[['name', 'nb_reviewers']]\n",
    "top10.index = np.linspace(1, 10, 10).astype(int)\n",
    "top10.nb_reviewers = top10.nb_reviewers.astype(int)\n",
    "\n",
    "# Display the Top 10 countries (considering US states) with the most reviewers\n",
    "print('Top 10 countries (considering US states) with the most reviewers:')\n",
    "display(top10)\n",
    "\n",
    "# Plot the choropleth map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 7))\n",
    "world_merge.boundary.plot(ax=ax, color='black', linewidth=0.5)\n",
    "world_merge.plot(column='nb_reviewers', ax=ax, legend=True,\n",
    "                 norm=LogNorm(vmin=1, vmax=world_merge['nb_reviewers'].max()),\n",
    "                 legend_kwds={'label': \"Number of Reviewers by Country\", 'orientation': \"vertical\"},\n",
    "                 cmap='Blues')\n",
    "\n",
    "# Remove the axis\n",
    "ax.set_title('Distribution of the reviewers in the World')\n",
    "ax.set_axis_off()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **the majority of reviewers are concentrated in the USA**. This observation will be a crucial factor in our subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Relative distance between Reviewers and Breweries**\n",
    "\n",
    "Now that we have examined the geographical distribution of breweries and reviewers, our next focus is to **explore the relative distance between the reviewer and the brewery for each review**. This analysis aims to provide insights into **how the popularity of a brewery is distributed globally**.\n",
    "\n",
    "To achieve this, we begin by calculating the centroid of each country (or state in the case of US). Subsequently, we add these centroid values to both the users and breweries dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add centroid to world dataset\n",
    "world_df = world_with_US_states.copy(deep=True)\n",
    "world_df['centroids'] = world_df['geometry'].centroid.to_crs(32633)\n",
    "world_df = world_df.set_index('name')\n",
    "display(world_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of users and breweries with centroids\n",
    "\n",
    "# Merge breweries with state centroids\n",
    "brew_cen = pd.merge(breweries_loc, world_df, left_on=['brewery_state'], right_on=['name'], how='inner')\n",
    "brew_cen.rename(columns={'centroids': 'brewery_centroid'}, inplace=True)\n",
    "\n",
    "# Merge breweries with country centroids\n",
    "brew_cen = pd.merge(brew_cen, world_df, left_on=['brewery_location'], right_on=['name'], how='inner')\n",
    "brew_cen.rename(columns={'centroids': 'brewery_centroid_country'}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "brew_cen = brew_cen.drop(['geometry_x', 'geometry_y', 'nbr_beers', 'brewery_name'], axis=1)\n",
    "\n",
    "# Merge users with state centroids\n",
    "user_cen = pd.merge(users_loc, world_df, left_on=['user_state'], right_on=['name'], how='inner')\n",
    "user_cen.rename(columns={'centroids': 'user_centroid'}, inplace=True)\n",
    "\n",
    "# Merge users with country centroids\n",
    "user_cen = pd.merge(user_cen, world_df, left_on=['user_location'], right_on=['name'], how='inner')\n",
    "user_cen.rename(columns={'centroids': 'user_centroid_country'}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "user_cen = user_cen.drop(['geometry_x', 'geometry_y', 'nbr_ratings', 'nbr_reviews', 'user_name', 'joined'], axis=1)\n",
    "\n",
    "# Display a sample of the resulting dataframes\n",
    "display(user_cen.sample(3))\n",
    "display(brew_cen.sample(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with reviews\n",
    "review_brew_user = reviews_filt[['brewery_id','user_id']].copy(deep=True)\n",
    "review_brew_user = pd.merge(review_brew_user,brew_cen,on=['brewery_id'],how='inner')\n",
    "review_brew_user = pd.merge(review_brew_user,user_cen,on=['user_id'],how='inner')\n",
    "display(review_brew_user.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataframe containing every review, along with the centroid of the reviewed brewery and the centroid of the user's residence, we can proceed to **compute the distance between the reviewer and the brewery**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_distance = review_brew_user.copy(deep=True)\n",
    "rev_distance['distance_state'] = rev_distance.apply(lambda row: row['brewery_centroid'].distance(row['user_centroid'])/1000, axis=1)\n",
    "\n",
    "rev_distance['distance_country'] = rev_distance.apply(lambda row: row['brewery_centroid_country'].distance(row['user_centroid_country'])/1000, axis=1)\n",
    "\n",
    "display(rev_distance.sample(10))\n",
    "#rev_distance['distance'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the distances between reviewer and brewery locations\n",
    "plt.hist(rev_distance['distance_country'], bins=np.linspace(0, 50000, 500), log=True)\n",
    "\n",
    "# Set x and y labels and the title\n",
    "plt.xlabel('Distance between reviewer location and brewery location [km]')\n",
    "plt.ylabel('Number of reviews')\n",
    "plt.title('Reviewers relative distance to breweries')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparison of Relative Distance with Size and Popularity Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge distance and metrics dataframes on 'brewery_id'\n",
    "metric_distance = pd.merge(rev_distance[['brewery_id', 'distance_country', 'distance_state']],\n",
    "                          df_metrics[['brewery_id', 'size_metrics', 'popularity_metrics']],\n",
    "                          on=['brewery_id'], how='inner')\n",
    "\n",
    "# Group by 'brewery_id' and calculate the median\n",
    "metric_distance = metric_distance.groupby('brewery_id').median()\n",
    "\n",
    "display(metric_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot scatter for 'size metrics' in the first subplot\n",
    "axs[0].scatter(metric_distance['size_metrics'], metric_distance['distance_country'], color='blue')\n",
    "axs[0].set_title('Distribution of the breweries based on size metrics')\n",
    "axs[0].set_ylabel('Mean distance of reviewers')\n",
    "axs[0].set_xlabel('Size metric')\n",
    "\n",
    "# Plot histogram for 'popularity metrics' in the second subplot\n",
    "axs[1].scatter(metric_distance['popularity_metrics'], metric_distance['distance_country'], )\n",
    "axs[1].set_title('Distribution of the breweries based on popularity metrics')\n",
    "axs[1].set_ylabel('Mean distance of reviewers')\n",
    "axs[1].set_xlabel('Popularity metric')\n",
    "\n",
    "axs[0].set_xscale('log')\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merge reviews with Users dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_aug = pd.merge(reviews_filt, users_loc, on=['user_id','user_name'], how='inner')\n",
    "display(reviews_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merge Reviews and Users with breweries dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RBU = pd.merge(reviews_aug, breweries_loc, on=['brewery_id','brewery_name'], how='inner')\n",
    "display(df_RBU)\n",
    "print('Number of NaN by category:')\n",
    "print(df_RBU.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Temoral Analysis on ratings and the number of reviews**\n",
    "\n",
    "With existing overview of the dataset, the next step is to investigate the temporal evolution of the beer market from the perspective of user ratings and reviews. Aligned with our interest in how brewery sizes matter, we pay specific attention to **how the ratings and reviews are distributed among breweries**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset columns of the dataframe and rename columns for clarification\n",
    "merged = df_RBU[['brewery_name', 'brewery_id', 'date','rating', 'text', 'nbr_ratings', 'nbr_reviews', 'joined']].copy()\n",
    "\n",
    "merged.rename(columns={'nbr_ratings': 'nbr_ratings_user',\n",
    "                       'nbr_reviews': 'nbr_reviews_user',\n",
    "                       'date': 'date_review',\n",
    "                       'joined': 'joined_user'\n",
    "                      }, inplace=True)\n",
    "\n",
    "# Extract year from the datetime\n",
    "# date_format = '%Y-%m-%d %H:%M:%S'\n",
    "merged['joined_user_year'] = merged.joined_user.apply(lambda x: x.year)\n",
    "merged['date_review_year'] = merged.date_review.apply(lambda x: x.year)\n",
    "\n",
    "# merge by the [year, brewery]\n",
    "merged_brewery_year = merged.groupby(['date_review_year','brewery_id']).apply(lambda x: pd.Series({\n",
    "    'brewery_nbr_reviews': x.size,\n",
    "    'brewery_avg_rating': x.rating.mean(),\n",
    "    'brewery_rating_list': x.rating.tolist()\n",
    "})).reset_index()\n",
    "\n",
    "print('Data for each brewery per year')\n",
    "merged_brewery_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge by the year of ratings for all breweries, preserving both macro (for breweries) and micro (equally treating reviews) info\n",
    "'''\n",
    "For each year::\n",
    "nbr_reviews: #reviews received for all breweries\n",
    "brewery_nbr_reviews: a list of #reviews for each brewery \n",
    "macro_avg_brewery_rating: the macro average rating of each brewery\n",
    "brewery_avg_rating_list: the list of each brewery's average rating\n",
    "ratings: all received rating scores\n",
    "'''\n",
    "merged_year = merged_brewery_year.groupby(['date_review_year']).apply(lambda x: pd.Series({\n",
    "    'nbr_reviews': x.brewery_nbr_reviews.sum(),\n",
    "    'brewery_nbr_reviews': x.brewery_nbr_reviews.tolist(),\n",
    "    'macro_avg_brewery_rating': x.brewery_avg_rating.mean(),\n",
    "    'brewery_avg_rating_list': x.brewery_avg_rating.tolist(),\n",
    "    'ratings': x.brewery_rating_list.sum()\n",
    "})).reset_index()\n",
    "\n",
    "merged_year['micro_avg_rating'] = merged_year.ratings.agg(lambda x: np.mean(x))\n",
    "print('Data for all breweries per year')\n",
    "merged_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics for visualization\n",
    "stats_by_year = pd.DataFrame({\n",
    "    'year': merged_year.date_review_year,\n",
    "        'macro_average': merged_year.macro_avg_brewery_rating,\n",
    "        'macro_std': merged_year.brewery_avg_rating_list.apply(lambda x: np.std(x)),\n",
    "        'micro_average': merged_year.micro_avg_rating,\n",
    "        'micro_std': merged_year.ratings.apply(lambda x: np.std(x)),\n",
    "        'nbr_reviews': merged_year.nbr_reviews,\n",
    "        'average_nbr_reviews': merged_year.brewery_nbr_reviews.apply(lambda x: np.mean(x)),\n",
    "        'nbr_reviews_std': merged_year.brewery_nbr_reviews.apply(lambda x: np.std(x)),\n",
    "        'nbr_reviews_percentage': merged_year[['nbr_reviews','brewery_nbr_reviews']].apply(lambda x: [y / x['nbr_reviews'] for y in x['brewery_nbr_reviews']], axis=1)\n",
    "})\n",
    "stats_by_year['percentage_mean'] = stats_by_year.nbr_reviews_percentage.apply(lambda x: np.mean(x))\n",
    "stats_by_year['percentage_median'] = stats_by_year.nbr_reviews_percentage.apply(lambda x: np.median(x))\n",
    "stats_by_year['percentage_std'] = stats_by_year.nbr_reviews_percentage.apply(lambda x: np.std(x))\n",
    "stats_by_year['nbr_brewery'] = stats_by_year.nbr_reviews_percentage.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5), dpi=100)\n",
    "plt.fill_between(stats_by_year.year, stats_by_year.macro_average - stats_by_year.macro_std,\n",
    "                 stats_by_year.macro_average + stats_by_year.macro_std, alpha = 0.2, color = 'C1')\n",
    "plt.plot(stats_by_year.year, stats_by_year.macro_average, color = 'C1', label='_nolegend_')\n",
    "plt.fill_between(stats_by_year.year, stats_by_year.micro_average - stats_by_year.micro_std,\n",
    "                 stats_by_year.micro_average + stats_by_year.micro_std, alpha = 0.2, color = 'C0')\n",
    "line_micro = plt.plot(stats_by_year.year, stats_by_year.micro_average, color = 'C0', label='_nolegend_')\n",
    "\n",
    "plt.ylim([2, 5])\n",
    "plt.xlim([1996, 2017])\n",
    "plt.xticks([1996,1998,2000,2002,2004,2006,2008,2010,2012,2014,2016,2017])\n",
    "plt.legend(['Macro Average','Micro Average'])\n",
    "plt.title('The average of all ratings vs the average across breweries')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Ratings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the trend average of ratings, we observe that the micro average is generally higher than the macro average across the breweries. It indicates scenario such as: a few breweries receiving relatively higher amount of high ratigns. This motivates us to explore further the data with respect to different sizes of popularities of breweries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = stats_by_year.year\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(9, 5), dpi=100)\n",
    "\n",
    "# color = 'C1'\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_xticks([1996,1998,2000,2002,2004,2006,2008,2010,2012,2014,2016,2017])\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xlim([1996,2017])\n",
    "ax1.set_ylabel('Percentage #ratings for breweries', color='chocolate')\n",
    "ax1.plot(stats_by_year.year, stats_by_year.percentage_mean, color = 'orange')\n",
    "ax1.plot(stats_by_year.year, stats_by_year.percentage_median, color = 'olive')\n",
    "plt.legend(['Mean of review percentages','Median of review percentages'], loc = 'upper center')\n",
    "# ax1.errorbar(stats_by_year.year, stats_by_year.percentage_mean,\n",
    "#              yerr = stats_by_year.percentage_std,\n",
    "#              capsize= 3, color = color)\n",
    "ax1.tick_params(axis='y', labelcolor='chocolate')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color2 = 'tab:blue'\n",
    "ax2.set_ylabel('#reviews', color=color2) \n",
    "ax2.set_xlim([1996,2017])\n",
    "ax2.plot(stats_by_year.year, stats_by_year.nbr_brewery, color = color2)\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "plt.legend(['#reviews'], loc='center')\n",
    "\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.title('The trend of percentage of reviews that breweries received & the total number of reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of reviews is increasing along the years, possibly due to the development of internet and the brewing process. The divergence of mean and median of percentage of reviews received by breweries again indicate an unenvenly distribution across breweries。\n",
    "\n",
    "From the decreasing curves of means and medians, we have two assumptions that should be validated later during mileston 3:\n",
    "* The number of breweries competing in the beer market is growing\n",
    "* In its early years, the BeerAdvocate website collected ratings for only a few breweries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_by_year_long = stats_by_year[stats_by_year.year >= 1997][['year','nbr_reviews_percentage']].copy()\n",
    "stats_by_year_long = stats_by_year_long.explode('nbr_reviews_percentage')\n",
    "# df_long['grades'] = pd.to_numeric(df_long['grades'])\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "ax = sns.boxplot(data=stats_by_year_long, x='year', y='nbr_reviews_percentage')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Percentage of #reviews')\n",
    "ax.set_title('The distribution of percentage of reviews received by breweries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the boxplot, it is interesting to observe massive number outliers with percentage of reviews higher than average, especially after the year of 2000. The boxplot indicates that while the mojority of breweries receive nearly zero reviews on BeerAdvocate website, there are a few breweries receiving much higher number of reviews. Therefore, we research question regarding this phenomenon is: \n",
    "\n",
    "**Does monopoly power dominates the beer industry? If yes, do those monopoly share common sizes and popularities?** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
