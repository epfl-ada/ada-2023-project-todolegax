{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<h1 align=\"center\"><strong>Does size matter?</strong></h1>\n",
    "  <p align=\"center\">\n",
    "    Applied Data Analysis (CS-401)\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers are increasingly relying on **product rating** websites to inform their purchasing decisions. It has been demonstrated that when customers rate a product, they often exhibit a **tendency to be influenced by the previous ratings** of other customers, a phenomenon known as the **_herding effect_**.\n",
    "\n",
    "Despite this, an unresolved research question revolves around comprehending **how ratings might be impacted by the scale and the reputation of the vendor**. Utilizing data sourced from beer reviews websites, our objective is to investigate the **connection** between the **size and fame of vendors** (specifically, breweries) and **the perceived quality** of their products.\n",
    "\n",
    "Through the quantification of brewery size and popularity using **predefined metrics** and the **extraction of sentiment** from textual reviews, our aim is to ascertain whether a correlation exists between vendor size and notoriety and perceived product quality. Additionally, we plan to **explore the behaviors** of diverse consumer bases, considering **temporal dimensions** (how these phenomena have evolved over the years and seasons within the same year) and **spatial dimensions** (how these relationships differ across states and countries).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BeerAdvocate**: Final Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import  libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import os\n",
    "from datetime import datetime\n",
    "import help_functions as helpfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data folder paths for BeerAdvocate\n",
    "DATA_FOLDER_BA = 'DATA/BeerAdvocate/'\n",
    "\n",
    "# Define the file paths for the datasets\n",
    "Beers_DATASET = DATA_FOLDER_BA+\"beers.csv\"\n",
    "Users_DATASET = DATA_FOLDER_BA+\"users.csv\"\n",
    "Reviews_DATASET = DATA_FOLDER_BA+\"reviews_BA.csv\"\n",
    "Breweries_DATASET = DATA_FOLDER_BA+\"breweries.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Exploration & Cleaning\n",
    "\n",
    "The first step of the project consits in exploring the data available in the dataset as well as pre-processing them. This step consists mainly in handling the potential missing data and reformatting what needs to be reformated. The following subsections present this step for every datasets available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Beers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We starts with the dataset containing information about beers. We will probably not use this datatset since our project focuses more on the reviews and breweries but we decided to still perform some pre-processing on it in the case where we suddenly need it during the analysis of Milestone 3. The dataset includes the following columns:\n",
    "\n",
    "- `beer_id`: Identifier for the beer.\n",
    "- `beer_name`: Name of the beer.\n",
    "- `brewery_id`: Identifier for the brewery producing the beer.\n",
    "- `brewery_name`: Name of the brewery.\n",
    "- `style`: Beer style.\n",
    "- `nbr_ratings`: Number of ratings received.\n",
    "- `nbr_reviews`: Number of reviews.\n",
    "- `avg`: Average rating.\n",
    "- `ba_score`: BeerAdvocate score.\n",
    "- `bros_score`: Bros score.\n",
    "- `abv`: Alcohol by volume.\n",
    "- `avg_computed`: Computed average rating.\n",
    "- `zscore`: Z-score.\n",
    "- `nbr_matched_valid_ratings`: Number of matched valid ratings.\n",
    "- `avg_matched_valid_ratings`: Average of matched valid ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the datasets into Pandas DataFrames\n",
    "beers = pd.read_csv(Beers_DATASET)\n",
    "# Display 2 random chosen samples of the set\n",
    "display(beers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset is composed of 280'823 beers. However, we can see that some data are missing and that some beers have 0 reviews. Furthermore, we should check if there is some duplicates. To address these issues, the following pre-processing steps are applied:\n",
    "\n",
    "- **Filtering Beers with Less than 5 Reviews**: Deleting beers with fewer than 5 reviews, as they may not be characteristic.\n",
    "\n",
    "- **Handling Missing Values**: Dropping rows with NaN values in the `nbr_ratings` column.\n",
    "\n",
    "- **Removing Duplicates**: Dropping duplicate entries based on the `beer_name` and `beer_id` columns.\n",
    "\n",
    "- **Column Selection**: Dropping columns that won't be used in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a minimum threshold for the number of reviews\n",
    "MIN_NUMBER_OF_REVIEWS = 5\n",
    "\n",
    "# Create a filtered copy of the 'beers' DataFrame with a minimum number of reviews\n",
    "beers_filt = beers.copy(deep=True)\n",
    "beers_filt = beers_filt[beers_filt['nbr_reviews'] >= MIN_NUMBER_OF_REVIEWS]\n",
    "\n",
    "# Remove rows with missing values in the 'nbr_reviews' column\n",
    "beers_filt = beers_filt[beers_filt['nbr_reviews'].notna()]\n",
    "\n",
    "# Drop duplicate entries based on the 'beer_name' column\n",
    "beers_filt = beers_filt.drop_duplicates(subset=['beer_name'])\n",
    "\n",
    "# Calculate the number of duplicate entries based on 'beer_name' and 'beer_id'\n",
    "dupli_name = np.sum(beers_filt.duplicated(subset=['beer_name']))\n",
    "dupli_ID = np.sum(beers_filt.duplicated(subset=['beer_id']))\n",
    "\n",
    "# Drop specific columns from the filtered DataFrame\n",
    "beers_filt = beers_filt.drop(['zscore', 'nbr_matched_valid_ratings', 'avg_matched_valid_ratings', 'bros_score', 'ba_score'], axis=1)\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "display(beers_filt)\n",
    "\n",
    "# Print the number of duplicate entries for 'beer_name' and 'beer_id'\n",
    "print(f'Number of duplicate beer name = {dupli_name}')\n",
    "print(f'Number of duplicate beer ID = {dupli_ID}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, the number of beers drops to 42'923."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move to the pre-processing of the dataset containing information about users. The dataset includes the following columns:\n",
    "\n",
    "- `nbr_ratings`: Number of ratings made.\n",
    "- `nbr_reviews`: Number of reviews done.\n",
    "- `user_id`: Unique user identifier.\n",
    "- `user_name`: User Name.\n",
    "- `joined`:  Date of the sign up.\n",
    "- `location`: Location of the User.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the datasets into Pandas DataFrames\n",
    "users = pd.read_csv(Users_DATASET)\n",
    "display(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed of 153'704 users. We can again see that some data are missing and that some users did not gave any review. To address these issues our pre-processing involves the following steps:\n",
    "\n",
    "- **Filtering Users with 0 number of reviews**: Deleting users with 0 reviews, as they are not characteristic.\n",
    "\n",
    "- **Handling Missing Values**: Dropping rows with NaN values in the `nbr_reviews`, `user_id`, `user_name` and `location` columns.\n",
    "\n",
    "- **Check for Duplicated Users**: Check if there are multiple user with the same id.\n",
    "\n",
    "- **Formatting the date**: Reformat the date in the column `joined` in UTC format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the 'users' dataframe\n",
    "users_filt = users.copy(deep=True)\n",
    "\n",
    "# Check for duplicates based on user name and user ID\n",
    "dupli_name = np.sum(users_filt.duplicated(subset=['user_name']))\n",
    "dupli_ID = np.sum(users_filt.duplicated(subset=['user_id']))\n",
    "\n",
    "# Remove users with 0 reviews and NaN as the number of reviews\n",
    "users_filt = users_filt[users_filt['nbr_reviews'] >= 1]\n",
    "users_filt = users_filt[users_filt['nbr_reviews'].notna()]\n",
    "\n",
    "# Remove rows with NaN in 'user_id', 'user_name', and 'location'\n",
    "users_filt = users_filt[users_filt['user_id'].notna()]\n",
    "users_filt = users_filt[users_filt['user_name'].notna()]\n",
    "users_filt = users_filt[users_filt['location'].notna()]\n",
    "\n",
    "# Convert 'joined' column to datetime type\n",
    "users_filt['joined'] = users_filt['joined'].apply(lambda x: datetime.utcfromtimestamp(x) if not pd.isna(x) else x)\n",
    "\n",
    "# Rename the 'location' column to 'user_location'\n",
    "users_filt.rename(columns={'location': 'user_location'}, inplace=True)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "display(users_filt)\n",
    "\n",
    "# Display the number of duplicate user names and user IDs\n",
    "print(f'Number of duplicate user names = {dupli_name}')\n",
    "print(f'Number of duplicate user IDs = {dupli_ID}')\n",
    "\n",
    "# Display the number of NaN values in each category\n",
    "print('Number of NaN by category:')\n",
    "print(np.sum(users_filt.isna()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, a total of 58'199 users with complete information are kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Breweries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we conduct the pre-processing of the dataset containing information about breweries. The dataset comprises the following columns:\n",
    "\n",
    "- `id`: Brewery identifier.\n",
    "- `location`: Location of the brewery.\n",
    "- `name`: Name of the brewery.\n",
    "- `nbr_beers`: Number of beers produced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the datasets into Pandas DataFrames\n",
    "breweries = pd.read_csv(Breweries_DATASET)\n",
    "display(breweries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed of 16'758 breweries. We can directly see that some breweries have 0 beers and are therefore not relevant for our analysis. There might be some missing values as well, thus we decided to perform the following pre_processing steps:\n",
    "\n",
    "- **Filtering Breweries with 0 number of beers**: Deleting breweries with 0 beers, as they are not characteristic.\n",
    "\n",
    "- **Handling Missing Values**: Dropping rows with NaN values in the `nbr_beers`.\n",
    "\n",
    "- **Removing Duplicates**: Dropping duplicate entries based on the `name` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the 'breweries' dataframe\n",
    "breweries_filt = breweries.copy(deep=True)\n",
    "\n",
    "# Check for duplicates based on ID\n",
    "dupli_ID = np.sum(breweries_filt.duplicated(subset=['id']))\n",
    "\n",
    "# Remove breweries with 0 beers and NaN values\n",
    "breweries_filt = breweries_filt[breweries_filt['nbr_beers'] >= 1]\n",
    "breweries_filt = breweries_filt[breweries_filt.notna()]\n",
    "\n",
    "# Remove duplicate entries based on brewery name\n",
    "breweries_filt = breweries_filt.drop_duplicates(subset='name')\n",
    "\n",
    "# Check for duplicates based on name\n",
    "dupli_name = np.sum(breweries_filt.duplicated(subset=['name']))\n",
    "\n",
    "# Rename columns for consistency\n",
    "breweries_filt.rename(columns={'name': 'brewery_name', 'id': 'brewery_id', 'location': 'brewery_location'}, inplace=True)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "display(breweries_filt)\n",
    "\n",
    "# Display the number of duplicate names and IDs\n",
    "print(f'Number of duplicate names = {dupli_name}')\n",
    "print(f'Number of duplicate IDs = {dupli_ID}')\n",
    "\n",
    "# Display the number of NaN values in each category\n",
    "print('Number of NaN by category:')\n",
    "print(np.sum(breweries_filt.isna()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing, 14'158 breweries are remaining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now delve into the BeerAdvocate Reviews dataset, focusing on reviews of various beers. The dataset contains the following columns:\n",
    "\n",
    "- `beer_name`: Name of the beer.\n",
    "- `beer_id`: Identifier for the beer.\n",
    "- `brewery_name`: Name of the brewery producing the beer.\n",
    "- `brewery_id`: Identifier for the brewery.\n",
    "- `style`: Beer style.\n",
    "- `abv`: Alcohol by volume.\n",
    "- `date`: Timestamp of the review.\n",
    "- `user_name`: Username of the reviewer.\n",
    "- `user_id`: Identifier for the user.\n",
    "- `appearance`: Rating for the beer's appearance.\n",
    "- `aroma`: Rating for the beer's aroma.\n",
    "- `palate`: Rating for the beer's palate.\n",
    "- `taste`: Rating for the beer's taste.\n",
    "- `overall`: Overall rating.\n",
    "- `rating`: Overall user rating.\n",
    "- `text`: Review text.\n",
    "\n",
    "Since the .txt file containing the reviews is huge, we first converted it into smaller files that we then regrouped in a .csv file. The code in `split_reviews.py` was used or this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Reviews dataset into a pandas DataFrame.\n",
    "reviews_BA = pd.read_csv(Reviews_DATASET)\n",
    "display(reviews_BA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pre-processing begins with the following initial steps:\n",
    "\n",
    "- **Converting Timestamps to Datetime**: We start by converting the 'date' column, which contains timestamps, into the datetime format. This conversion enables us to perform time-based analyses more effectively.\n",
    "\n",
    "- **Handling Missing Values**: We address missing values in the dataset by dropping rows with NaN values. This ensures that our analysis is based on complete and reliable data.\n",
    "\n",
    "- **Removing Unnecessary Column**: The 'abv' column, representing the alcohol by volume, is not useful for our specific analysis. Consequently, we opt to drop this column to streamline our dataset.\n",
    "\n",
    "- **Removing White Space before and after Stings**: Some strings columns like `user_name` begin with a white space, which is a problem for merging. Consequently we need to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the reviews_BA DataFrame to avoid modifying the original DataFrame.\n",
    "reviews_filt = reviews_BA.copy(deep=True)\n",
    "# Convert the 'date' column to a datetime format.\n",
    "# If the 'date' value is not NaN, apply the conversion using utcfromtimestamp.\n",
    "# If the 'date' value is NaN, leave it unchanged.\n",
    "reviews_filt['date'] = reviews_filt['date'].apply(lambda x: datetime.utcfromtimestamp(x) if not pd.isna(x) else x)\n",
    "# Drop rows where the 'text' column has NaN values.\n",
    "reviews_filt = reviews_filt[reviews_filt['text'].notna()]\n",
    "# Drop the 'abv' column from the reviews_filt DataFrame.\n",
    "reviews_filt = reviews_filt.drop(['abv'], axis=1)\n",
    "# Remove leading and trailing whitespaces (if they exist) from the following columns: \n",
    "# user_id, user_name, beer_name, brewery_name, style, and text.\n",
    "reviews_filt.user_id = reviews_filt.user_id.apply(lambda x: x.strip())\n",
    "reviews_filt.user_name = reviews_filt.user_name.astype(str).apply(lambda x: x.strip())\n",
    "reviews_filt.beer_name = reviews_filt.beer_name.apply(lambda x: x.strip())\n",
    "reviews_filt.brewery_name = reviews_filt.brewery_name.apply(lambda x: x.strip())\n",
    "reviews_filt['style'] = reviews_filt['style'].apply(lambda x: x.strip())\n",
    "reviews_filt.text = reviews_filt.text.apply(lambda x: x.strip())\n",
    "# Display the updated reviews_filt DataFrame.\n",
    "display(reviews_filt)\n",
    "# Print the number of NaN values for each column in the reviews_filt DataFrame.\n",
    "print('Number of NaN by category:')\n",
    "print(np.sum(reviews_filt.isna()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now aim to visualize the distribution of the length of the reviews to:\n",
    "\n",
    "- **Get insight into review length variation**: Visualizing the distribution allows us to understand the range and variability in review lengths. Some reviews may be succinct, while others may be more detailed.\n",
    "\n",
    "- **Assess data quality**: Analyzing review lengths can also serve as a quality check. Unusually short or long reviews may warrant further investigation to ensure data integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the review lengths using the 'text' column from the reviews_filt DataFrame.\n",
    "plt.hist(reviews_filt['text'].str.len(), bins=200, log=True)\n",
    "\n",
    "# Set x-axis and y-axis labels and the title.\n",
    "plt.xlabel('Review length')\n",
    "plt.ylabel('Number of reviews')\n",
    "plt.title('Distribution of Review Length')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BeerAdvocate website advises to create reviews of at list 150 characters. It can be seen that not all the reviews have at least 150 characters. We will therefore remove them in the next steps to keep only relevant revies. The statistics of the review length are displayed in the next cell. As we can see, the median is at 580 characters. The distribution is skewed though, with a small quantity of reviews being more than 5000 characters. This will be interesting to analyse in the next milestone if there is a link between the length of the reviews and the scale or popularity of a brewery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics of the review lengths.\n",
    "reviews_filt['text'].str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted to improve consistency by **translating** all non-English textual reviews. To this end, we used the language detection module, $\\texttt{detect}$, of the $\\texttt{langdetect}$ library to **initially identify the language of each review**.\n",
    "\n",
    "Due to the considerable computation time required for language detection, we decided to **keep the language identifier** of each review in a separate dataset, together with the **unique identifiers** of the **beer** and the **user**.\n",
    "\n",
    "This approach allows us to store the language information in our archive, facilitating efficient access without the need to calculate the language detection for each review each time.\n",
    "\n",
    "Note that if the CSV file containing `user_id`, `beer_id` and `text_lang` (the language identifier of the reviews) exists in the repository, we can avoid recomputing the information. Instead, we can merge the review dataset with this auxiliary dataset into a consolidated dataset, simplifying our analysis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next sections will need these modules to be run\n",
    "# Importing the 'unescape' function from the 'html' module for text cleaning of html escape characters\n",
    "try:\n",
    "    from html import unescape\n",
    "except:\n",
    "    !pip install html\n",
    "\n",
    "# Importing the 'detect' function from the 'langdetect' module for language detection of reviews\n",
    "try:\n",
    "    from langdetect import detect\n",
    "except:\n",
    "    !pip install langdetect\n",
    "\n",
    "# Importing the 'GoogleTranslator' from the 'deep_translatore' module for reviews translation\n",
    "try:\n",
    "    from deep_translator import GoogleTranslator\n",
    "except:\n",
    "    !pip install deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the .csv file exists then we don't redo the detection\n",
    "data_name = 'reviews_lang.csv'\n",
    "CODE_ERROR = 'Error'\n",
    "\n",
    "if os.path.exists(data_name):\n",
    "    reviews_language = pd.read_csv(data_name)\n",
    "    reviews_filt = pd.merge(reviews_filt, reviews_language, on=['beer_id','user_id'], how='left')\n",
    "else:\n",
    "    # Detect the language of each review. Handle exception for non corrected reviews.\n",
    "    text_lang = []\n",
    "    for review in reviews_filt['text']:\n",
    "        try:\n",
    "            text_lang.append(detect(review))\n",
    "        except:\n",
    "            text_lang.append(CODE_ERROR)\n",
    "            continue\n",
    "    \n",
    "    # Adding a new column 'text_lang' to store the detected language for each review\n",
    "    reviews_filt['text_lang'] = pd.Series(text_lang)\n",
    "\n",
    "    # Store the language information in a  \n",
    "    reviews_filt[['beer_id', 'user_id', 'text_lang']].to_csv(data_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the consolidated dataset with the new column giving the language of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 3 randomly chosen rows of the new consolidated dataset\n",
    "display(reviews_filt.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check if errors occured during the detection, as well as if NaN values appeared in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of errors detected are:', np.sum(reviews_filt['text_lang'] == CODE_ERROR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of NaN values in text_lang colum is:', np.sum(reviews_filt['text_lang'].isna()), '/', len(reviews_filt['text_lang']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then decide to drop the NaN values due to the small number of occurrences of NaN values in the `text_lang` column in our filtered reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt = reviews_filt[reviews_filt['text_lang'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now have a look to the variety of languages composing the reviews. As expected a vast majority of them are in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The count and variety of distinct languages used in the reviews within our dataset:')\n",
    "print(reviews_filt['text_lang'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then filter rows in the DataFrame where the `text_lang` column is not 'en', and then apply translation to English for the corresponding 'text' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt[reviews_filt.text_lang != 'en']['text'] = reviews_filt[reviews_filt.text_lang != 'en']['text'].apply(lambda x: helpfn.translate_to_english(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all reviews now in **English**, we can proceed to remove the `text_lang` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt = reviews_filt.drop(['text_lang'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then undertook further pre-processing, focusing on the textual representation of reviews. We followed the following steps:\n",
    "\n",
    "- **Management of special characters**: After examining the modified and translated dataset, we observed the presence of some special characters such as \"\\&quot;\" and \"\\x92\" in some reviews. To solve this problem, we used the html.unescape function to convert the HTML entities and then removed the non-ASCII characters by encoding them in ASCII and decoding them again.\n",
    "\n",
    "- **Filtering short reviews**: As a final step, we filtered out reviews with less than 150 characters. This step aimed to exclude shorter reviews from our dataset, focusing on more substantial texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 'unescape' function to decode HTML entities in the 'text' column\n",
    "reviews_filt['text'] = reviews_filt['text'].apply(unescape)\n",
    "\n",
    "# Remove non-ASCII characters by encoding to ASCII and decoding back\n",
    "reviews_filt['text'] = reviews_filt['text'].apply(lambda x: x.encode('ascii', 'ignore').decode())\n",
    "\n",
    "# Set the minimum number of characters for reviews\n",
    "MIN_NUMBER_OF_CHARACTER = 150\n",
    "\n",
    "# Filter out reviews with fewer than 'min_character' characters\n",
    "reviews_filt = reviews_filt[reviews_filt['text'].str.len() > MIN_NUMBER_OF_CHARACTER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reviews_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with 2'587'598 reviews. Now that the datasets are ready, we can start with some preliminary analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Preliminary Analysis\n",
    "The website offers data since 1996. However we should consider that at the very begining of the website only ridiculously small portion of people had access to internet. Therefore we do some analysis of the evolution of the number of revoruenavui  blabla bla\n",
    "\n",
    "Plot of nbr revies and breweries evolution through years.\n",
    "\n",
    "--> conclude that we only keep things from ?? 2000 ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Temporal Evolution of # breweries and # reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt['date']= reviews_filt['date'].apply(lambda x: x.year)\n",
    "temp_df = reviews_filt[['brewery_id','date']].copy(deep=True)\n",
    "reviews_by_year = temp_df.groupby('date').size().reset_index(name='tot_nbr_reviews_by_year')\n",
    "brew_by_year = temp_df.groupby('date')['brewery_id'].unique().reset_index(name='brewery')\n",
    "brew_by_year['tot_nbr_brew_by_year'] = brew_by_year['brewery'].apply(lambda x: len(x))\n",
    "temporal_df = pd.merge(reviews_by_year,brew_by_year, on=['date'],how='inner')\n",
    "temporal_df['rev_by_brew'] = temporal_df.apply(lambda row: row.tot_nbr_reviews_by_year/row.tot_nbr_brew_by_year, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'date' if it's not sorted already\n",
    "df = temporal_df.sort_values(by='date')\n",
    "\n",
    "# Define pastel colors\n",
    "pastel_blue = '#AED6F1'\n",
    "pastel_red = '#F1948A'\n",
    "pastel_green = '#ABEBC6'\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Bar plot for total number of reviews by year\n",
    "bars1 = ax1.bar(df['date'], df['tot_nbr_reviews_by_year'], color=pastel_blue, alpha=0.7, edgecolor='black')\n",
    "ax1.set_title('Total Number of Reviews by Year')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Total Number of Reviews')\n",
    "for bar in bars1:\n",
    "    yval = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "# Bar plot for total number of breweries by year\n",
    "bars2 = ax2.bar(df['date'], df['tot_nbr_brew_by_year'], color=pastel_red, alpha=0.7, edgecolor='black')\n",
    "ax2.set_title('Total Number of Breweries Reviewed by Year')\n",
    "ax2.set_xlabel('Year')\n",
    "ax2.set_ylabel('Total Number of Breweries')\n",
    "for bar in bars2:\n",
    "    yval = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "# Bar plot for ratio\n",
    "bars3 = ax3.bar(df['date'], df['rev_by_brew'], color=pastel_green, alpha=0.7, edgecolor='black')\n",
    "ax3.set_title('Ratio between number of reviews and number of breweries by year')\n",
    "ax3.set_xlabel('Year')\n",
    "ax3.set_ylabel('#reviews/#breweries')\n",
    "for bar in bars3:\n",
    "    yval = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "\n",
    "# Adjust layout for better appearance\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt = reviews_filt[reviews_filt['date']>=2002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Metrics Definition\n",
    "In our analysis, we introduce **two key metrics** to quantitatively **assess the characteristics of breweries**: Size Metrics and Popularity Metrics. These metrics are created in the next cells. Since we don't have data about the revenue or number of liter produced by the brewery, we decided to base ourself on variable present in the dataset to construct these metrics. Some verification will be done by hand, by searching information about some breweries on the web to assess the quality of the metrics and to correct the coefficients if needed.\n",
    "\n",
    "####  **Size Metrics**\n",
    "To numerically evaluate the size of a brewery over the years, we built an index based on the following formula:\n",
    "\n",
    "$$ \\text{Size} = \\alpha \\log (N_r) + \\beta \\log (N_b) + \\gamma \\log (N_t)$$\n",
    "\n",
    "With:\n",
    "- $N_r =$ number of reviews received\n",
    "- $N_b =$ number of beers produced\n",
    "- $N_t =$ number of different types (style) of beer produced\n",
    "\n",
    "The coefficients $\\alpha, \\beta, \\gamma$ are chosen by hand. The metrics is computed for each brewery every year. For each year the value obtained for each brewery is normalized by the maximum value of the year, obtaining thus a value between 0 and 1 for each brewery, with the biggest brewery having a size index of 1.\n",
    "\n",
    "#### **Popularity Metrics**\n",
    "$$ \\text{Popularity} = \\dfrac{\\log (N_r)}{N_b}$$\n",
    "\n",
    "With $N_r =$ number of reviews, $N_b =$ number of beers produced\n",
    "\n",
    "\n",
    "Note that both metrics are normalized to get a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract brewery_id, style, beer_id and date  columns from reviews_filt\n",
    "review_brew = reviews_filt[['brewery_id', 'style', 'beer_id', 'date', 'rating']].copy(deep=True)\n",
    "\n",
    "reviews_filt['text'].str.len()\n",
    "\n",
    "review_brew['len_text'] = reviews_filt['text'].apply(lambda x: len(x))\n",
    "\n",
    "# Group by brewery ID and Year of review and compute nbr of reviews by year\n",
    "brew_review_score = review_brew.groupby(['brewery_id','date']).size().reset_index(name='nbr_reviews')\n",
    "\n",
    "# Count the number of unique styles for each brewery by year\n",
    "brew_style_score = review_brew.groupby(['brewery_id','date'])['style'].unique().reset_index(name='nbr_styles')\n",
    "brew_style_score['nbr_styles'] = brew_style_score['nbr_styles'].apply(lambda x: len(x))\n",
    "\n",
    "# Count the number of unique beer reviewed for each brewery by year\n",
    "brew_beer_score = review_brew.groupby(['brewery_id','date'])['beer_id'].unique().reset_index(name='nbr_beers')\n",
    "brew_beer_score['nbr_beers'] = brew_beer_score['nbr_beers'].apply(lambda x: len(x))\n",
    "\n",
    "# Compute average ratings for each brewery by year\n",
    "brew_rating_score = review_brew.groupby(['brewery_id','date'])['rating'].mean().reset_index(name='avg_rating')\n",
    "\n",
    "# Compute the mean review length for each brewery by year\n",
    "brew_mean_len = review_brew.groupby(['brewery_id','date'])['len_text'].mean().reset_index(name='mean_len_text')\n",
    "\n",
    "# Merge the review count, style count, and selected columns from breweries_filt\n",
    "brew_by_year = pd.merge(brew_review_score, brew_style_score[['brewery_id', 'date', 'nbr_styles']], left_on=['brewery_id','date'] , right_on=['brewery_id','date'], how='inner')\n",
    "brew_by_year = pd.merge(brew_by_year, brew_beer_score[['brewery_id', 'date', 'nbr_beers']], left_on=['brewery_id','date'] , right_on=['brewery_id','date'], how='inner')\n",
    "brew_by_year = pd.merge(brew_by_year, brew_rating_score[['brewery_id', 'date', 'avg_rating']], left_on=['brewery_id','date'] , right_on=['brewery_id','date'], how='inner')\n",
    "brew_by_year = pd.merge(brew_by_year, brew_mean_len[['brewery_id', 'date', 'mean_len_text']], left_on=['brewery_id','date'] , right_on=['brewery_id','date'], how='inner')\n",
    "brew_by_year = pd.merge(brew_by_year, reviews_by_year, on=['date'], how='inner')\n",
    "\n",
    "# Drop if only 1 review\n",
    "brew_by_year = brew_by_year[brew_by_year['nbr_reviews']>=2]\n",
    "display(brew_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_metrics = brew_by_year.copy(deep=True)\n",
    "# /row.tot_nbr_reviews_by_year ??\n",
    "df_metrics['size_metrics'] = df_metrics.apply(lambda row: 5*np.log(row.nbr_reviews) + np.log(row.nbr_styles) + 2*np.log(row.nbr_beers), axis = 1)\n",
    "# Apply the normalization within each year\n",
    "df_metrics['size_metrics'] = df_metrics.groupby('date')['size_metrics'].transform(helpfn.normalize_column)\n",
    "\n",
    "df_metrics['popularity_metrics'] = df_metrics.apply(lambda row: np.log(row.nbr_reviews)/row.nbr_beers, axis = 1)\n",
    "df_metrics['popularity_metrics'] = df_metrics.groupby('date')['popularity_metrics'].transform(helpfn.normalize_column)\n",
    "display(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_metric_macro = df_metrics.groupby('brewery_id')['size_metrics'].mean().reset_index(name='size_metrics_macro')\n",
    "popularity_metric_macro = df_metrics.groupby('brewery_id')['popularity_metrics'].mean().reset_index(name='popularity_metrics_macro')\n",
    "avg_rating_macro = df_metrics.groupby('brewery_id')['avg_rating'].mean().reset_index(name='avg_rating_macro')\n",
    "avg_len_text_macro = df_metrics.groupby('brewery_id')['mean_len_text'].mean().reset_index(name='avg_len_text_macro')\n",
    "df_metrics_macro = pd.merge(size_metric_macro,popularity_metric_macro, on=['brewery_id'], how='inner')\n",
    "df_metrics_macro = pd.merge(df_metrics_macro,avg_rating_macro, on=['brewery_id'], how='inner')\n",
    "df_metrics_macro = pd.merge(df_metrics_macro,avg_len_text_macro, on=['brewery_id'], how='inner')\n",
    "df_metrics_macro = pd.merge(df_metrics_macro,breweries_filt[['brewery_id','brewery_name']],on=['brewery_id'], how='inner')\n",
    "\n",
    "\n",
    "display(df_metrics_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot histogram for 'size metrics' in the first subplot\n",
    "axs[0].hist(df_metrics_macro['size_metrics_macro'], bins=20,color=pastel_blue, edgecolor='black')\n",
    "axs[0].set_title('Distribution of the breweries based on size metrics')\n",
    "axs[0].set_xlabel('Size metrics')\n",
    "axs[0].set_ylabel('Number of breweries')\n",
    "\n",
    "# Plot histogram for 'popularity metrics' in the second subplot\n",
    "axs[1].hist(df_metrics_macro['popularity_metrics_macro'], bins=20, color=pastel_green, edgecolor='black')\n",
    "axs[1].set_title('Distribution of the breweries based on popularity metrics')\n",
    "axs[1].set_xlabel('Popularity metrics')\n",
    "axs[1].set_ylabel('Number of breweries')\n",
    "\n",
    "# Set y-axis scale to logarithmic for better visibility of distribution in both subplots\n",
    "#axs[0].set_yscale('log')\n",
    "#axs[1].set_yscale('log')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Popularity metrics.................... But let's see if the biggest breweries are also the most popular ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort breweries by size\n",
    "size_sorted = df_metrics_macro.sort_values(by='size_metrics_macro', ascending=False).reset_index()\n",
    "size_sorted.index = np.linspace(1, len(size_sorted), len(size_sorted)).astype(int)\n",
    "print(\"Overview of the size metrics ranking:\")\n",
    "\n",
    "# Display a summary of the top breweries based on size metrics\n",
    "display(size_sorted[['brewery_name', 'size_metrics_macro']].iloc[[0, 1, 2, 9, 99, 999]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sorting the breweries by size, it becomes evident that the top-ranking breweries are primarily located in the US. This observation aligns with expectations, considering that the majority of reviewers and breweries are located in the US (as detailed in the following section).\n",
    "\n",
    "Looking for the 3 biggest and the 100th on the web, we can extract their beer production:\n",
    "\n",
    "- (1st) Boston Beer Company: 5,300,000 barrels (6,200,000 hL) found under https://en.wikipedia.org/wiki/Boston_Beer_Company\n",
    "- (2nd) Sierra Nevada Brewing Co.: 1,250,000 barrels (510,000 hL) found under https://en.wikipedia.org/wiki/Sierra_Nevada_Brewing_Company\n",
    "- (3rd) Stone Brewing: 325,645 barrels (382,000 hL) found under https://en.wikipedia.org/wiki/Stone_Brewing_Co.\n",
    "- (100th) Uinta Brewing Company: 77'000 barrels found under https://en.wikipedia.org/wiki/Uinta_Brewing_Company\n",
    "\n",
    "Given these values, it seems that the size metrics works, at least for the top ranked breweries.\n",
    "\n",
    "On the other side, the popularity metrics yield significantly different results. The top-ranked breweries include historically renowned Belgian breweries such as \"Brasserie de Rochefort,\" notable for its \"Trappist\" beer, and \"Brasserie d'Orval.\" This highlights that the popularity metric captures a different aspect of brewery influence, focusing on factors beyond sheer size.\n",
    "\n",
    "It is important to note that this preliminary analysis does not take into account the time evolution. It will be interesting to see how it has evolved over the years. Some breweries for example could have significantly changed size over the years. However this pre-analysis validates the choice of the metrics and opens a large range of possibilities for the next analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Evolution of the Size Metrics over the Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how brewery size distribution has changed over the years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_size_year = df_metrics.groupby('date')['size_metrics'].mean()\n",
    "std_size_year = df_metrics.groupby('date')['size_metrics'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the brewery size metrics definition\n",
    "years = df_metrics['date'].drop_duplicates()\n",
    "plt.figure(figsize=(8, 6))\n",
    "h,_,_,sc=plt.hist2d(df_metrics.date, df_metrics.size_metrics, norm=LogNorm(), bins=[np.linspace(2001.5, 2017.5, 17),25], cmap='Reds')\n",
    "plt.errorbar(years, mean_size_year, yerr=std_size_year, fmt='o-', color='white', ecolor='white',label='Yearly mean with std of size metrics', capsize=5, alpha=1.0)\n",
    "\n",
    "plt.colorbar(sc)\n",
    "plt.ylabel('Size metrics')\n",
    "plt.xlabel('years')\n",
    "plt.legend()\n",
    "plt.title('Evolution of the size metrics distribution according to time [years]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of brewery sizes has remained fairly stable over the years. Nevertheless, since around 2014, the proportion of small breweries has tended to increase substantially. This growth can be partly explained by the strong interest in breweries and the creation of craft beers observed between 2015 and 2020. Google searches for the term brewery, for example, experienced strong increases during this period, which matches the results given by our size metric.\n",
    "\n",
    "![Brewery Google query time evolution](images/google_search_brewery.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Small, medium, big breweries\n",
    "Grouping by small, medium and big breweries according to the size metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_macro['size_category'] = df_metrics_macro['size_metrics_macro'].apply(helpfn.categorize_size)\n",
    "df_metrics['size_category'] = df_metrics['size_metrics'].apply(helpfn.categorize_size)\n",
    "\n",
    "display(df_metrics_macro)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the breweries are divide into the size categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the percentage of breweries by size categories\n",
    "perc_small_breweries = (df_metrics_macro['size_category']=='small').sum() / df_metrics_macro.shape[0] * 100\n",
    "perc_medium_breweries = (df_metrics_macro['size_category']=='medium').sum() / df_metrics_macro.shape[0] * 100\n",
    "perc_big_breweries = (df_metrics_macro['size_category']=='big').sum() / df_metrics_macro.shape[0] *100\n",
    "\n",
    "print(f\"Percentage of small breweries: {perc_small_breweries:.2f}\",'%')\n",
    "print(f\"Percentage of medium breweries: {perc_medium_breweries:.2f}\",'%')\n",
    "print(f\"Percentage of big breweries: {perc_big_breweries:.2f}\",'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the vast majority of breweries are small and only a minority are big breweries, probably with an international reach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Geographical Data\n",
    "\n",
    "The goal of this section is to gain insights into the geographical distribution of both breweries and reviewers within the dataset. Ultimately, we aim to calculate the distances between breweries and their respective reviewers. This analysis could potentially unveil distinctions between brewery types, revealing whether certain types of breweries attract predominantly local reviewers or have a more globally dispersed audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Spatial Distribution of Breweries\n",
    "\n",
    "We initiate our analysis by examining the geographical distribution of breweries in the dataset. To achieve this, we integrate the dataset with a map sourced from Geopandas (https://www.naturalearthdata.com/downloads/110m-cultural-vectors/).\n",
    "\n",
    "Ensuring alignment between the country names used in the map and those in the brewery dataset is crucial. To address this, we calculate the Hamming distance between them and substitute the brewery location with the closest match. In instances where no match is found, we opt to eliminate the corresponding brewery. This scenario applies to 26 breweries out of over 14,000, which is deemed acceptable for this level of analysis. It's worth noting that some removed breweries had HTTP links as their location, explaining the lack of match with the map.\n",
    "\n",
    "For breweries located in the USA, the dataset includes information about the state. Consequently, we extend the same process to the states in the United States of America. To facilitate this, we introduce a new column, 'state,' in the dataframe. This allows us to split the location of US breweries into 'country' and 'state.' \n",
    "\n",
    "The geodataframe for the USA can be accessed here: https://eric.clst.org/tech/usgeojson/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make deep copy of breweries filtered dataframe reset indexes (needed for the loop next)\n",
    "breweries_loc = breweries_filt.copy(deep=True)\n",
    "breweries_loc.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Load the world map shapefile\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# Load the USA state map\n",
    "us_states = gpd.read_file('GeoJSON/gz_2010_us_040_00_500k.json')\n",
    "# Change name of Georgia into Georgia USA (sae for Puerto Rico)\n",
    "us_states.loc[28,'NAME'] = 'Georgia USA'\n",
    "us_states.loc[16,'NAME'] = 'Puerto Rico USA'\n",
    "# Extract countries name\n",
    "unique_loc = world['name'].unique()\n",
    "# Extract state name\n",
    "unique_state = us_states['NAME'].unique()\n",
    "\n",
    "# Create column for state in breweries df, will be used for USA\n",
    "breweries_loc['state'] = '-'\n",
    "breweries_loc.rename(columns={'brewery_location': 'country'}, inplace=True)\n",
    "\n",
    "breweries_loc, dropped_brew = helpfn.match_country_name(breweries_loc,\n",
    "                                                 unique_loc,\n",
    "                                                 unique_state)\n",
    "\n",
    "# Display a sample of the modified dataframe\n",
    "breweries_loc.rename(columns={'country':'brewery_country','state':'brewery_state'}, inplace=True)\n",
    "display(breweries_loc.sample(3))\n",
    "\n",
    "# Print the number of dropped breweries and their locations\n",
    "print(f'{len(dropped_brew)} breweries were dropped, at the following location:')\n",
    "print(dropped_brew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore the **Top 10 countries** with the **highest number of breweries** to gain insights into the **global distribution of brewing establishments**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of breweries in each country\n",
    "brewery_counts = breweries_loc['brewery_country'].value_counts().reset_index()\n",
    "brewery_counts.columns = ['brewery_country', 'nb_breweries']\n",
    "\n",
    "# Identify the top 10 countries\n",
    "top10 = brewery_counts.sort_values(by='nb_breweries', ascending=False).head(10)\n",
    "top10 = top10[['brewery_country', 'nb_breweries']]\n",
    "top10.index = np.linspace(1, 10, 10).astype(int)\n",
    "top10.nb_breweries = top10.nb_breweries.astype(int)\n",
    "\n",
    "# Display the Top 10 countries with the most breweries\n",
    "print('Top 10 countries with the most breweries:')\n",
    "display(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the data, **there are significantly more breweries in the US** compared to other countries. Due to this notable concentration, we will consider the breweries by states in the next section. To facilitate this, we extend the world geodataframe by incorporating the US state geodataframe in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add US states to world df\n",
    "us_states.rename(columns={'NAME': 'name'}, inplace = True)\n",
    "world_with_US_states = pd.concat([world[['name','geometry']],us_states[['name','geometry']]])\n",
    "duplicated_rows = world_with_US_states['name'].duplicated(keep='first')\n",
    "\n",
    "# Drop rows with duplicate values in Column1\n",
    "world_with_US_states = world_with_US_states[~duplicated_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look again to the Top 10 countries with the highest number of breweries. However, this time, we'll consider **each US state as a distinct 'country'.** Additionally, we'll **visualize the global distribution of breweries** to gain a comprehensive understanding of their geographical spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of breweries in each country\n",
    "brewery_counts = breweries_loc['brewery_state'].value_counts().reset_index()\n",
    "brewery_counts.columns = ['brewery_state', 'nb_breweries']\n",
    "\n",
    "# Merge brewery counts with the world map data\n",
    "world_merge = world_with_US_states.merge(brewery_counts, how='left', left_on='name', right_on='brewery_state')\n",
    "\n",
    "# Fill NaN values (countries without breweries) with 0\n",
    "world_merge['nb_breweries'].fillna(0, inplace=True)\n",
    "# Fill NaN values of state with Unknown\n",
    "world_merge['brewery_state'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Find top 10 countries\n",
    "top10 = world_merge.sort_values(by='nb_breweries', ascending=False).head(10)\n",
    "top10 = top10[['name', 'nb_breweries']]\n",
    "top10.index = np.linspace(1, 10, 10).astype(int)\n",
    "top10.nb_breweries = top10.nb_breweries.astype(int)\n",
    "\n",
    "# Display the Top 10 countries (considering US states) with the most breweries\n",
    "print('Top 10 countries (considering US states) with the most breweries:')\n",
    "display(top10)\n",
    "\n",
    "# Plot the choropleth map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 7))\n",
    "world_merge.boundary.plot(ax=ax, color='black', linewidth=0.5)\n",
    "world_merge.plot(column='nb_breweries', ax=ax, legend=True,\n",
    "                 norm=LogNorm(vmin=1, vmax=world_merge['nb_breweries'].max()),\n",
    "                 legend_kwds={'label': \"Number of Breweries by Country\",\n",
    "                              'orientation': \"vertical\"},\n",
    "                 cmap='Reds')\n",
    "\n",
    "# Remove the axis\n",
    "ax.set_title('Distribution of the breweries in the World')\n",
    "ax.set_axis_off()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The revised analysis reveals distinct results. Germany takes the lead, followed by Finland, and several US states are now present in the Top 10. The conclusion of this analysis is that there are many more breweries in the US than anywhere else in the world. Acknowledging this, it becomes essential to consider the US's internal diversity in subsequent analyses. Given the availability of information about individual US states, incorporating them into the analysis could provide valuable insights, offering a more nuanced understanding of the brewing landscape within the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Spatial Distribution of the Users\n",
    "Now that we have gained insights into the distribution of breweries worldwide, we can apply a **similar analysis** to the **user data**. The aim of this analysis is to see whether or not a geographical similarity can be observed between the distribution of breweries and the distribution of users who give reviews. Once again, a matching process based on Hamming's distance must be performed to match the names of the different places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the filtered users dataset and reset indexes (needed for the loop next)\n",
    "users_loc = users_filt.copy(deep=True)\n",
    "users_loc.reset_index(inplace=True,drop=True)\n",
    "# Create a column for the state in the users dataframe, to be used for USA\n",
    "users_loc['state'] = '-'\n",
    "users_loc.rename(columns={'user_location': 'country'}, inplace=True)\n",
    "# Init dropped breweries\n",
    "#dropped_brew = []\n",
    "users_loc, dropped_user = helpfn.match_country_name(users_loc,\n",
    "                                             unique_loc,\n",
    "                                             unique_state)\n",
    "\n",
    "# Display a sample of the modified dataframe\n",
    "users_loc.rename(columns={'country':'user_country','state':'user_state'}, inplace=True)\n",
    "\n",
    "# Display a sample of the modified users dataframe\n",
    "display(users_loc.sample(3))\n",
    "\n",
    "# Print the number of dropped users and their locations\n",
    "print(f'{len(dropped_user)} users were dropped, at the following location:')\n",
    "print(dropped_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the **Top 10 countries** with the **highest number of reviewers** to gain insights into the global distribution of reviewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of reviewers in each country\n",
    "reviewers_counts = users_loc['user_country'].value_counts().reset_index()\n",
    "reviewers_counts.columns = ['user_country', 'nb_reviewers']\n",
    "\n",
    "# Identify the top 10 countries according to the number of reviewers\n",
    "top10 = reviewers_counts.sort_values(by='nb_reviewers', ascending=False).head(10)\n",
    "top10 = top10[['user_country', 'nb_reviewers']]\n",
    "top10.index = np.linspace(1, 10, 10).astype(int)\n",
    "top10.nb_reviewers = top10.nb_reviewers.astype(int)\n",
    "\n",
    "# Display the Top 10 countries with the most reviewers\n",
    "print('Top 10 countries with the most reviewers:')\n",
    "display(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, there is once again a **substantial number of reviewers in the US**. The distribution seems fairly similar to that observed for breweries. In the next section, we will **replicate the analysis**, this time considering reviewers based on **US states**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of reviewers in each country\n",
    "reviewers_counts = users_loc['user_state'].value_counts().reset_index()\n",
    "reviewers_counts.columns = ['user_state', 'nb_reviewers']\n",
    "\n",
    "# Merge reviewer counts with the world map data\n",
    "world_merge = world_with_US_states.merge(reviewers_counts, how='left', left_on='name', right_on='user_state')\n",
    "\n",
    "# Fill NaN values (countries without reviewers) with 0\n",
    "world_merge['nb_reviewers'].fillna(0, inplace=True)\n",
    "world_merge['user_state'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Identify the top 10 countries\n",
    "top10 = world_merge.sort_values(by='nb_reviewers', ascending=False).head(10)\n",
    "top10 = top10[['name', 'nb_reviewers']]\n",
    "top10.index = np.linspace(1, 10, 10).astype(int)\n",
    "top10.nb_reviewers = top10.nb_reviewers.astype(int)\n",
    "\n",
    "# Display the Top 10 countries (considering US states) with the most reviewers\n",
    "print('Top 10 countries (considering US states) with the most reviewers:')\n",
    "display(top10)\n",
    "\n",
    "# Plot the choropleth map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 7))\n",
    "world_merge.boundary.plot(ax=ax, color='black', linewidth=0.5)\n",
    "world_merge.plot(column='nb_reviewers', ax=ax, legend=True,\n",
    "                 norm=LogNorm(vmin=1, vmax=world_merge['nb_reviewers'].max()),\n",
    "                 legend_kwds={'label': \"Number of Reviewers by Country\", 'orientation': \"vertical\"},\n",
    "                 cmap='Blues')\n",
    "\n",
    "# Remove the axis\n",
    "ax.set_title('Distribution of the reviewers in the World')\n",
    "ax.set_axis_off()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **the majority of reviewers are concentrated in the USA**. In fact, not only the majority of breweries but also the vast majority of reviewers are located in the USA. This is certainly a crucial factor to take into account in future analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Global Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Ratings vs Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now analyze if and how the size metric impacts the average rating obtained by the breweries. \n",
    "A priori, one might expect the scores obtained by small breweries to be fairly evenly distributed, ranging from very bad to very good, depending on taste, with potentially more polarizing flavours. On the other hand, in the case of large breweries with an international reach, we might expect the scores to be more grouped, reflecting products that are more polished and more complex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_analysis = df_metrics_macro.copy(deep=True)\n",
    "\n",
    "# Compute the log of the avg. rating for each breweries\n",
    "df_rating_analysis['log_avg_rating'] = df_metrics_macro['avg_rating_macro'].apply(lambda x : np.log(x))\n",
    "df_rating_analysis['log_size_metrics'] = df_metrics_macro['size_metrics_macro'].apply(lambda x : np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "point_size = 2\n",
    "\n",
    "# Plot scatter 'size_metrics_macro'\n",
    "plt.scatter(df_rating_analysis['size_metrics_macro'], df_rating_analysis['avg_rating_macro'], color='cornflowerblue', label='avg. rating', s=point_size)\n",
    "\n",
    "plt.title('Distribution of Ratings w.r.t. Size and Popularity Metrics')\n",
    "plt.ylabel('Avg. rating obtained')\n",
    "plt.xlabel('Metrics')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the distribution is highly dispersed for the smaller breweries, sweeping almost the entire range of scores, whereas the average score obtained by the larger breweries seems to be slightly higher and less spread out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discretize the size metrics by interval of size 0.05 in order to class the breweries into categories and see the rating trend with respect to the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size interval width\n",
    "interval_width = 0.05\n",
    "\n",
    "# Calculate the minimum and maximum size values to determine the range\n",
    "min_size = df_rating_analysis['size_metrics_macro'].min()\n",
    "max_size = df_rating_analysis['size_metrics_macro'].max()\n",
    "\n",
    "# Create intervals for size\n",
    "size_intervals = pd.interval_range(start=min_size, end=max_size, freq=interval_width)\n",
    "\n",
    "# Assign each row to its corresponding interval\n",
    "df_rating_analysis['interval'] = pd.cut(df_rating_analysis['size_metrics_macro'], bins=size_intervals)\n",
    "\n",
    "mean_std_rating_by_size = df_rating_analysis.groupby('interval')['avg_rating_macro'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "display(mean_std_rating_by_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Middle points of the size metric intervals\n",
    "mean_std_rating_by_size['midpoints'] = mean_std_rating_by_size['interval'].apply(lambda x: x.mid)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(len(mean_std_rating_by_size)):\n",
    "    if i % 2 == 0:\n",
    "        plt.axvspan(mean_std_rating_by_size['interval'][i].left, mean_std_rating_by_size['interval'][i].right, color='lightgray', alpha=0.5)\n",
    "    \n",
    "plt.scatter(df_rating_analysis['size_metrics_macro'], df_rating_analysis['avg_rating_macro'], color='#AED6F1',label='avg. rating', s=point_size)\n",
    "plt.errorbar(mean_std_rating_by_size['midpoints'], mean_std_rating_by_size['mean'], yerr=mean_std_rating_by_size['std'], fmt='o-', color='b', ecolor='cornflowerblue', label='mean and std rating for each size segment', capsize=5)\n",
    "plt.title(f'Average rating by {interval_width:.2f} size categories with Standard deviation')\n",
    "plt.xlabel('Size metric (segmented)')\n",
    "plt.ylabel('Avg. rating')\n",
    "\n",
    "plt.legend()\n",
    "#plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a fairly linear increase in the mean rating obtained as the size metric increases, as well as a tightening of the standard deviation. According to that, being a larger brewery would therefore tend to increase the chances of obtaining a higher overall rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether a linear relationship can be discerned between the size metric and the average rating obtained by the breweries, we run a linear regression on the raw distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_continous = smf.ols(formula='avg_rating_macro ~ size_metrics_macro', data=df_rating_analysis).fit()\n",
    "print(model_continous.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does seem to be a linear correlation between having a large size index and the average rating obtained by a brewery. However, the $R^2 = 0.065$ value indicates that this regression explains the correlation rather poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the case where the rating is averaged by metric size, the linear correlation is, as expected, much stronger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_discret = sm.OLS(mean_std_rating_by_size['mean'], mean_std_rating_by_size['midpoints']).fit()\n",
    "print(model_discret.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we must remain cautious with these results, as it is likely that other cofounders, not taken into account, have an impact on the average rating obtained by the breweries. It is therefore likely that the size index does not explain everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Rating vs Size Categories\n",
    "\n",
    "Now that the distribution and general trend of the average rating in relation to the size index has been observed, it is interesting to categorize the breweries into 3 different sizes, small, medium and big in order to see if belonging to a of these categories explains the overall score in a statistically significant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for means and standard deviations\n",
    "means, std_devs = helpfn.compute_stats_for_categories(df_rating_analysis,'avg_rating_macro')\n",
    "labels = ['Small', 'Medium', 'Big']\n",
    "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
    "dark_colors  = ['darkgreen', 'darkblue', 'indianred']\n",
    "size = [0, 0.3, 0.7, 1.0]\n",
    "mid = [0.15, 0.5, 0.85]\n",
    "point_size = 3\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels = ['Small breweries', 'Medium breweries', 'Big breweries']\n",
    "text_positions = zip(mid, [1.5, 1.5, 1.5], labels, dark_colors)\n",
    "\n",
    "for i in range(len(size)-1):\n",
    "    plt.axvspan(size[i], size[i+1], color=colors[i], alpha=0.5)\n",
    "    plt.errorbar(mid[i], means[i], yerr=std_devs[i], fmt='o-', color=dark_colors[i], ecolor=dark_colors[i], label='mean and std rating for '+labels[i], capsize=5, alpha=1.0)\n",
    "    #plt.scatter(mid[i], means[i], color=dark_colors[i], s=100)  # Adding points at text positions\n",
    "    for pos in text_positions:\n",
    "        plt.text(pos[0], pos[1], pos[2], color=pos[3], ha='center', va='center', fontsize=14)\n",
    "\n",
    "plt.scatter(df_rating_analysis['size_metrics_macro'], df_rating_analysis['avg_rating_macro'], color='darkgray',label='avg. rating distribution', s=point_size)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Avg. rating w.r.t. Size metrics with mean rating and std. for the size categories')\n",
    "plt.xlabel('Size metrics')\n",
    "plt.ylabel('Avg. rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Average rating for the small breweries: {:.3g} with std: {:.3g}'.format(means[0], std_devs[0]))\n",
    "print('Average rating for the medium breweries: {:.3g} with std: {:.3g}'.format(means[1], std_devs[1]))\n",
    "print('Average rating for the big breweries: {:.3g} with std: {:.3g}'.format(means[2], std_devs[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that belonging to a larger category tends to increase the average rating obtained quite significantly. An average rating difference of almost 0.4 can be observed between small and large breweries. However, we must be careful, as the standard deviation remains quite high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a difference over the years ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now legitimate to question the temporal robustness of this result. Can the same conclusion be drawn in 2005, 2010 and 2016? To investigate this, the same analysis of the average rating on the 3 size categories over these years is carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporal = df_metrics.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_list = [2005, 2010, 2016]\n",
    "dfs = {} \n",
    "means = {}\n",
    "stds = {}\n",
    "\n",
    "for year in years_list:\n",
    "    dfs[f\"df_small_breweries_{year}\"] = df_temporal[(df_temporal['size_category']=='small') & (df_temporal['date']==year)]\n",
    "    dfs[f\"df_medium_breweries_{year}\"] = df_temporal[(df_temporal['size_category']=='medium') & (df_temporal['date']==year)]\n",
    "    dfs[f\"df_big_breweries_{year}\"] = df_temporal[(df_temporal['size_category']=='big') & (df_temporal['date']==year)]\n",
    "\n",
    "    means[f\"mean_small_{year}\"] = dfs[f'df_small_breweries_{year}']['avg_rating'].mean()\n",
    "    stds[f\"std_small_{year}\"] = dfs[f'df_small_breweries_{year}']['avg_rating'].std()\n",
    "\n",
    "    means[f\"mean_medium_{year}\"] = dfs[f'df_medium_breweries_{year}']['avg_rating'].mean()\n",
    "    stds[f\"std_medium_{year}\"] = dfs[f'df_medium_breweries_{year}']['avg_rating'].std()\n",
    "\n",
    "    means[f\"mean_big_{year}\"] = dfs[f'df_big_breweries_{year}']['avg_rating'].mean()\n",
    "    stds[f\"std_big_{year}\"] = dfs[f'df_big_breweries_{year}']['avg_rating'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['small', 'medium', 'big']\n",
    "labels = ['Small breweries', 'Medium breweries', 'Big breweries']\n",
    "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
    "dark_colors = ['darkgreen', 'darkblue', 'indianred']\n",
    "size = [0, 0.3, 0.7, 1.0]\n",
    "mid = [0.15, 0.5, 0.85]\n",
    "\n",
    "def plot_on_subplot(ax, j):\n",
    "    for i in range(len(size) - 1):\n",
    "        ax.axvspan(size[i], size[i + 1], color=colors[i], alpha=0.5)\n",
    "        ax.errorbar(mid[i], means[f\"mean_{categories[i]}_{years_list[j]}\"], yerr=stds[f\"std_{categories[i]}_{years_list[j]}\"], fmt='o-', color=dark_colors[i],\n",
    "                    ecolor=dark_colors[i],label='Mean and std rating ' + str(categories[i]),\n",
    "                    capsize=5, alpha=1.0)\n",
    "        if j == 0:\n",
    "            ax.scatter(dfs[f\"df_{categories[i]}_breweries_{years_list[j]}\"]['size_metrics'],\n",
    "                    dfs[f\"df_{categories[i]}_breweries_{years_list[j]}\"]['avg_rating'], color='darkgray',\n",
    "                   label='rating distribution', s=point_size)\n",
    "        else:\n",
    "            ax.scatter(dfs[f\"df_{categories[i]}_breweries_{years_list[j]}\"]['size_metrics'],\n",
    "                    dfs[f\"df_{categories[i]}_breweries_{years_list[j]}\"]['avg_rating'], color='darkgray', s=point_size)\n",
    "        ax.text(mid[i], 1.7, labels[i], color=dark_colors[i], ha='center', va='center', fontsize=12)\n",
    "\n",
    "        ax.set_title('Avg. rating distribution ' + str(years_list[j]))\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_ylim([1.5,5])\n",
    "        ax.set_xlim([0,1])\n",
    "        ax.set_xlabel('Size metrics')\n",
    "        ax.set_ylabel('Avg. rating')\n",
    "        \n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))  # Creating three subplots side by side\n",
    "\n",
    "for j,ax in enumerate(axs):\n",
    "    plot_on_subplot(ax,j)\n",
    "    \n",
    "\n",
    "plt.legend(loc=(0.4,0.25))\n",
    "plt.suptitle('Avg. rating distribution w.r.t. ize metric with mean yearly rating and std. for the size categories and for years 2005, 2010, 2016')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print mean and std for each size categories and years\n",
    "for year in years_list:\n",
    "    print(f\"Mean rating for the small breweries in {year}: {means[f'mean_small_{year}']:.4g} with std: {stds[f'std_small_{year}']:.4g}\")\n",
    "    print(f\"Mean rating for the medium breweries in {year}: {means[f'mean_medium_{year}']:.4g} with std: {stds[f'std_medium_{year}']:.4g}\")\n",
    "    print(f\"Mean rating for the big breweries in {year}: {means[f'mean_big_{year}']:.4g} with std: {stds[f'std_big_{year}']:.4g}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the same conclusion as in section 3.1.2 applies to all years. Belonging to a bigger category would tend to increase the average rating obtained. There is therefore no major temporal evolution in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final investigation is to test whether there really is a statistically significant difference in terms of average rating when belonging to one size category rather than another. To do this, a **Student's t-test** is performed on the average rating between the *small/medium* and *medium/big* categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform t-test anlysis at the 0.05 significance level under the hypotesis $H_0$ : *There is no statistically significant difference in average rating between the breweries size categories.* To see if the size category of a brewery impacts the average rate obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.ttest_ind(df_rating_analysis[df_rating_analysis['size_category']=='small']['avg_rating_macro'],\n",
    "                                       df_rating_analysis[df_rating_analysis['size_category']=='medium']['avg_rating_macro'], equal_var=False)\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "print(f\"T-statistic: {t_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.8}\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"The difference in avg. rating is statistically significant between small and medium breweries.\")\n",
    "else:\n",
    "    print(\"The difference in avg. rating is not statistically significant between small and medium breweries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.ttest_ind(df_rating_analysis[df_rating_analysis['size_category']=='medium']['avg_rating_macro'],\n",
    "                                       df_rating_analysis[df_rating_analysis['size_category']=='big']['avg_rating_macro'], equal_var=False)\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "print(f\"T-statistic: {t_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.8f}\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"The difference in avg. rating is statistically significant between medium and big breweries.\")\n",
    "else:\n",
    "    print(\"The difference in avg. rating is not statistically significant between medium and big breweries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that there is likely to be a significant (at significance level 0.05) statisticall difference between the small, medium and large breweries in term of rating obtained. Given the plot just on top, being a bigger breweries tend to increase the mean rating obtained and reduce the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Review length vs. brewery size analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same analysis but to see if the brewery size metrics impacts the average review length obtained by the breweries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_len = df_metrics_macro.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for means and standard deviations\n",
    "means_rew_len, std_devs_rew_len = helpfn.compute_stats_for_categories(df_review_len,'avg_len_text_macro')\n",
    "\n",
    "labels = ['Small', 'Medium', 'Big']\n",
    "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
    "dark_colors  = ['darkgreen', 'darkblue', 'indianred']\n",
    "size = [0, 0.3, 0.7, 1.0]\n",
    "mid = [0.15, 0.5, 0.85]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "labels = ['Small breweries', 'Medium breweries', 'Big breweries']\n",
    "text_positions = zip(mid, [2500, 2500, 2500], labels, dark_colors)\n",
    "\n",
    "for i in range(len(size)-1):\n",
    "    plt.axvspan(size[i], size[i+1], color=colors[i], alpha=0.5)\n",
    "    plt.errorbar(mid[i], means_rew_len[i], yerr=std_devs_rew_len[i], fmt='o-', color=dark_colors[i], ecolor=dark_colors[i],label='mean and std review len. for '+labels[i], capsize=5, alpha=1.0)\n",
    "    \n",
    "    #plt.scatter(mid[i], means[i], color=dark_colors[i], s=100)  # Adding points at text positions\n",
    "    for pos in text_positions:\n",
    "        plt.text(pos[0], pos[1], pos[2], color=pos[3], ha='center', va='center', fontsize=14)\n",
    "\n",
    "plt.scatter(df_review_len['size_metrics_macro'], df_review_len['avg_len_text_macro'], color='darkgray', label='review len distribution', s=point_size)\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Avg. review length w.r.t. Size metric with mean review lenght and std. for the size categories')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Size metric')\n",
    "plt.ylabel('Avg. review length')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Averge review length for the small breweries: ', means_rew_len[0], 'with std: ', std_devs_rew_len[0])\n",
    "print('Averge review length for the medium breweries: ', means_rew_len[1], 'with std: ', std_devs_rew_len[1])\n",
    "print('Averge review length for the big breweries: ', means_rew_len[2], 'with std: ', std_devs_rew_len[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypothesis testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform t-test anlysis at the 0.05 significance level under the hypotesis $H_0$ : *There is no statistically significant difference in average review length between the breweries size categories.* To see if the size category of a brewery impacts the average review length obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.ttest_ind(df_review_len[df_review_len['size_category']=='small']['avg_len_text_macro'],\n",
    "                                       df_review_len[df_review_len['size_category']=='medium']['avg_len_text_macro'], equal_var=False)\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "print(f\"T-statistic: {t_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.8f}\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"The difference in avg. review length is statistically significant between small and medium breweries.\")\n",
    "else:\n",
    "    print(\"The difference in avg. review length is not statistically significant between small and medium breweries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.ttest_ind(df_review_len[df_review_len['size_category']=='medium']['avg_len_text_macro'],\n",
    "                                       df_review_len[df_review_len['size_category']=='big']['avg_len_text_macro'], equal_var=False)\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "print(f\"T-statistic: {t_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.8f}\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"The difference in avg. review length is statistically significant between medium and big breweries.\")\n",
    "else:\n",
    "    print(\"The difference in avg. review length is not statistically significant between medium and big breweries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Geographical Distribution of Reviewers vs Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Relative distance between Reviewers and Breweries\n",
    "\n",
    "Our next focus is to **explore the relative distance between the reviewer and the brewery for each review**. This analysis aims to provide insights into **how the popularity of a brewery is distributed globally**.\n",
    "\n",
    "To achieve this, we begin by calculating the centroid of each country (or state in the case of US). Subsequently, we can compute the distance between the countries/state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add centroid to world dataset\n",
    "world_df = world_with_US_states.copy(deep=True)\n",
    "world_df = world_df.to_crs(3857)\n",
    "world_df['centroids'] = world_df['geometry'].centroid.to_crs(4326)\n",
    "world_df = world_df.reset_index(drop=True)\n",
    "display(world_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store distances\n",
    "distances_df = pd.DataFrame(index=world_df['name'], columns=world_df['name'])\n",
    "\n",
    "# Calculate distances for each pair of countries\n",
    "for i in range(len(world_df)):\n",
    "    for j in range(i + 1, len(world_df)):\n",
    "        country1 = world_df.loc[i, 'name']\n",
    "        country2 = world_df.loc[j, 'name']\n",
    "        lat1, lon1 = gpd.GeoSeries(world_df.loc[i, 'centroids']).y, gpd.GeoSeries(world_df.loc[i, 'centroids']).x\n",
    "        lat2, lon2 = gpd.GeoSeries(world_df.loc[j, 'centroids']).y, gpd.GeoSeries(world_df.loc[j, 'centroids']).x\n",
    "        \n",
    "        # Compute haversine distance\n",
    "        distance = helpfn.haversine(lat1, lon1, lat2, lon2)\n",
    "        \n",
    "        # Fill both entries since the distance is symmetric\n",
    "        distances_df.at[country1, country2] = distance\n",
    "        distances_df.at[country2, country1] = distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the brewery/user location and state with reviews\n",
    "review_brew_user = reviews_filt[['brewery_id','user_id','date']].copy(deep=True)\n",
    "review_brew_user = pd.merge(review_brew_user,breweries_loc[['brewery_id','brewery_country','brewery_state']],on=['brewery_id'],how='inner')\n",
    "review_brew_user = pd.merge(review_brew_user,users_loc[['user_id','user_country','user_state']],on=['user_id'],how='inner')\n",
    "display(review_brew_user.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataframe containing every review, along with the location of the reviewed brewery and the user's residence, we can add the distance computed above in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_distance = review_brew_user.copy(deep=True)\n",
    "# Create new column with distance between reviewer and the brewery for each review\n",
    "rev_distance['distance_state'] = rev_distance.apply(lambda row: distances_df.loc[row.brewery_state,row.user_state],axis=1)\n",
    "# Replace the NaN by 0, (the distance between the same country is set to NaN by defintion (distance(belgium,belgium) = NaN))\n",
    "rev_distance = rev_distance.fillna(0)\n",
    "display(rev_distance.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the consistency of the distance, we use the tool from Google Maps to compute distance. We can see below that our measure is pretty accurate. The computed distance between Belgium and Ohio is 6504 km wheras the tool from Google Maps shows 6518 km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_distance.iloc[2269445]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img title=\"Distance between Belgium and Ohio\" alt=\"Distance between Belgium and Ohio\" src=\"images/Be_Ohio.png\" width=\"800\"></center>\n",
    "<center>Distance between Belgium and Ohio (Google Maps)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_dist = rev_distance.copy(deep=True)\n",
    "# Plot a histogram of the distances between reviewer and brewery locations\n",
    "plt.hist(rev_dist['distance_state'], bins=100, color='Blue')\n",
    "\n",
    "# Set x and y labels and the title\n",
    "plt.xlabel('Distance between reviewer location and brewery location [km]')\n",
    "plt.ylabel('Number of reviews')\n",
    "plt.title('Reviewers relative distance to breweries')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph showing the distribution of relative distances between reviewers and breweries for each review is quite diversified, with a majority of user reviewing \"local\" beers (that comes from the same state/country), and then a large proportion of reviews between 0 and 8000km, corresponding notably to intra-USA reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Is there a relation between the size and the spatial distribution of the reviewers ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now interesting to look at the relative distance of reviewers for a given brewery based on its **size**. This investigation would make it possible to discern whether a small local brewery receives a majority of reviews from nearby users, or whether a world-renowned brewery receives international reviews, resulting in a higher average distance. Since the vast majority of the reviewers are located in the USA, we decided to restrain this analysis to the USA, meaning that we will only consider the breweries on the american ground. This is is necessary since the non-american breweries will receive most of their reviews from american people and thus their distance will be bigger.\n",
    "\n",
    "First of all, we need to populate the distance dataframe with information on brewery size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge distance and metrics dataframes on 'brewery_id'\n",
    "metric_distance = pd.merge(rev_dist[['brewery_id', 'distance_state','brewery_country','user_country']],\n",
    "                          df_metrics_macro[['brewery_id', 'size_metrics_macro', 'popularity_metrics_macro','size_category']],\n",
    "                          on=['brewery_id'], how='inner')\n",
    "\n",
    "display(metric_distance.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only US breweries\n",
    "USA_metric_dist = metric_distance[metric_distance['brewery_country']=='United States of America']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look to the mean distance of the reviewers for each brewery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'brewery_id' and calculate the mean distance of the reviewers\n",
    "USA_mean = USA_metric_dist.groupby('brewery_id').agg({\n",
    "    'brewery_country': 'first', \n",
    "    'distance_state': 'mean',\n",
    "    'user_country': 'first', \n",
    "    'size_metrics_macro': 'first',\n",
    "    'size_category': 'first',\n",
    "    'popularity_metrics_macro': 'first'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for means and standard deviations\n",
    "means, std_devs = helpfn.compute_stats_for_categories(USA_mean,'distance_state')\n",
    "labels = ['Small', 'Medium', 'Big']\n",
    "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
    "dark_colors  = ['darkgreen', 'darkblue', 'indianred']\n",
    "size = [0, 0.3, 0.7, 1.0]\n",
    "mid = [0.15, 0.5, 0.85]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels = ['Small breweries', 'Medium breweries', 'Big breweries']\n",
    "text_positions = zip(mid, [5000, 5000, 5000], labels, dark_colors)\n",
    "\n",
    "for i in range(len(size)-1):\n",
    "    plt.axvspan(size[i], size[i+1], color=colors[i], alpha=0.5)\n",
    "    plt.errorbar(mid[i], means[i], yerr=std_devs[i], fmt='o-', color=dark_colors[i], ecolor=dark_colors[i], label='mean and std distance for '+labels[i], capsize=5, alpha=1.0)\n",
    "    #plt.scatter(mid[i], means[i], color=dark_colors[i], s=100)  # Adding points at text positions\n",
    "    for pos in text_positions:\n",
    "        plt.text(pos[0], pos[1], pos[2], color=pos[3], ha='center', va='center', fontsize=14)\n",
    "\n",
    "plt.scatter(USA_mean['size_metrics_macro'], USA_mean['distance_state'], color='darkgray',label='Breweries', s=3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Avg. review distance w.r.t. Size metric with mean rating and std. for the size categories')\n",
    "plt.xlabel('Size metric')\n",
    "plt.ylabel('Avg. Distance')\n",
    "plt.xlim([0,1])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Macro-average of the reviewer distance for the small breweries: ', means[0], 'with std: ', std_devs[0])\n",
    "print('Macro-average of the reviewer distance for the medium breweries: ',means[1], 'with std: ', std_devs[1])\n",
    "print('Macro-average of the reviewer distance for the big breweries: ', means[2], 'with std: ', std_devs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot is very interesting. We can see that a big portion of the small breweries has a mean distance to their reviewer close to zero. Meaning that their reviewers are living in the same state. This is not the case for bigger breweries, that seem to be reviewed by a more global panel of reviewers. It would be interesting to see now if this results holds over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge distance and metrics dataframes on 'brewery_id'\n",
    "yearly_metric_distance = pd.merge(rev_dist[['brewery_id', 'distance_state','brewery_country','user_country','date']],\n",
    "                          df_metrics[['brewery_id','date', 'size_metrics','size_category']],\n",
    "                          on=['brewery_id','date'], how='inner')\n",
    "# Keep only Us breweries\n",
    "USA_yearly_metric_distance = yearly_metric_distance[yearly_metric_distance['brewery_country']=='United States of America']\n",
    "display(yearly_metric_distance.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'brewery_id' and calculate the mean distance of the reviewers\n",
    "years = [2005, 2010, 2016]\n",
    "dfs = []\n",
    "for year in years:\n",
    "    dfs.append(USA_yearly_metric_distance[USA_yearly_metric_distance['date']==year].groupby('brewery_id').agg({\n",
    "        'brewery_country': 'first', \n",
    "        'distance_state': 'mean',\n",
    "        'user_country': 'first', \n",
    "        'size_metrics': 'first',\n",
    "        'size_category': 'first',\n",
    "        'date': 'first'\n",
    "    }).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Small', 'Medium', 'Big']\n",
    "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
    "dark_colors  = ['darkgreen', 'darkblue', 'indianred']\n",
    "size = [0, 0.3, 0.7, 1.0]\n",
    "mid = [0.15, 0.5, 0.85]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "labels = ['Small breweries', 'Medium breweries', 'Big breweries']\n",
    "text_positions = zip(mid, [5200, 5200, 5200], labels, dark_colors)\n",
    "for j,df in enumerate(dfs):\n",
    "    # Create lists for means and standard deviations\n",
    "    means, std_devs = helpfn.compute_stats_for_categories(dfs[j],'distance_state')\n",
    "\n",
    "    for i in range(len(size)-1):\n",
    "\n",
    "        axs[j].axvspan(size[i], size[i+1], color=colors[i], alpha=0.5)\n",
    "        axs[j].errorbar(mid[i], means[i], yerr=std_devs[i], fmt='o-', color=dark_colors[i], ecolor=dark_colors[i], label='mean and std distance for '+labels[i], capsize=5, alpha=1.0)\n",
    "        for p,pos in enumerate(text_positions):\n",
    "            axs[j].text(pos[0], pos[1], pos[2], color=pos[3], ha='center', va='center', fontsize=11)\n",
    "\n",
    "    axs[j].scatter(df['size_metrics'], df['distance_state'], color='darkgray',label='Breweries', s=3)\n",
    "    text_positions = zip(mid, [5200, 5200, 5200], labels, dark_colors)\n",
    "    axs[j].set_title(f'Avg. review distance w.r.t. Size metric \\n {years[j]}')\n",
    "    axs[j].set_xlabel('Size metric')\n",
    "    axs[j].set_ylabel('Avg. Distance')\n",
    "    axs[j].set_xlim([0,1])\n",
    "    axs[j].set_ylim([-500,7500])\n",
    "    axs[j].grid(True,axis='y',linestyle=':')\n",
    "\n",
    "axs[2].legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brew2002 = df_metrics.copy(deep=True)\n",
    "brew2017 = df_metrics.copy(deep=True)\n",
    "brew2002 = brew2002[brew2002['date']==2002]\n",
    "brew2017 = brew2017[brew2017['date']==2017]\n",
    "big2017 = brew2017[brew2017['size_category']=='big']\n",
    "small2002 = brew2002[brew2002['size_category']=='small']\n",
    "small2big = pd.merge(small2002,big2017,on=['brewery_id'], how='inner')\n",
    "big2002 = brew2002[brew2002['size_category']=='big']\n",
    "small2002 = pd.merge(small2002['brewery_id'],df_metrics[['brewery_id','date','size_metrics']],on=['brewery_id'], how='inner')\n",
    "big2002 = pd.merge(big2002['brewery_id'],df_metrics[['brewery_id','date','size_metrics']],on=['brewery_id'], how='inner')\n",
    "display(small2big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot using seaborn\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "sns.lineplot(x='date', y='size_metrics', hue='brewery_id', data=small2002, marker='o', ax=axs[0])\n",
    "axs[0].set_title('Evolution of Breweries that were small in 2002')\n",
    "axs[0].set_xlabel('Year')\n",
    "axs[0].set_ylabel('Size Metrics')\n",
    "#axs[0].legend(title='Brewery ID')\n",
    "sns.lineplot(x='date', y='size_metrics', hue='brewery_id', data=big2002, marker='o', ax=axs[1])\n",
    "axs[1].set_title('Evolution of Breweries that were big in 2002')\n",
    "axs[1].set_xlabel('Year')\n",
    "axs[1].set_ylabel('Size Metrics')\n",
    "#axs[1].legend(title='Brewery ID')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example = df_metrics[df_metrics['brewery_id']==1199].copy(deep=True)\n",
    "USA_dist = USA_yearly_metric_distance[['brewery_id','date','distance_state']][USA_yearly_metric_distance['brewery_id']==1199].copy(deep=True)\n",
    "\n",
    "USA_dist = USA_dist.groupby('date').agg({\n",
    "        'brewery_id': 'first',\n",
    "        'distance_state': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "Example = pd.merge(Example,breweries_loc[['brewery_id','brewery_name']],on=['brewery_id'], how='inner')\n",
    "Example = pd.merge(Example,USA_dist[['brewery_id','date','distance_state']], on=['brewery_id','date'], how='inner')\n",
    "display(Example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
